<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 5: Probability Inequalities</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 50%, #ff6b6b 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '‚â§‚â•‚â§‚â•‚â§‚â•‚â§‚â•‚â§‚â•';
            position: absolute;
            top: 50%;
            left: -20%;
            transform: translateY(-50%);
            font-size: 6em;
            opacity: 0.1;
            animation: scroll 15s linear infinite;
        }

        @keyframes scroll {
            0% { left: -20%; }
            100% { left: 120%; }
        }

        .header h1 {
            position: relative;
            z-index: 1;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header p {
            position: relative;
            z-index: 1;
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 40px;
            padding: 30px;
            border-radius: 15px;
            background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
            border-left: 5px solid #ff6b6b;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .section::before {
            content: '';
            position: absolute;
            top: -100%;
            right: -100%;
            width: 200px;
            height: 200px;
            background: radial-gradient(circle, rgba(255,107,107,0.1) 0%, transparent 70%);
            border-radius: 50%;
            animation: pulse 4s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); opacity: 0.3; }
            50% { transform: scale(1.2); opacity: 0.1; }
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(255, 107, 107, 0.2);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #ff6b6b;
            padding-bottom: 10px;
            display: inline-block;
            position: relative;
            z-index: 1;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
            background: linear-gradient(135deg, #ff6b6b, #ee5a52);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            position: relative;
            z-index: 1;
        }

        .inequality-type {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 25px rgba(255, 107, 107, 0.3);
            transition: all 0.3s ease;
            position: relative;
        }

        .inequality-type::before {
            content: '‚â§';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 3em;
            opacity: 0.3;
        }

        .inequality-type:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 30px rgba(255, 107, 107, 0.4);
        }

        .inequality-type h4 {
            font-size: 1.4em;
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }

        .formula-box {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #f39c12;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            box-shadow: 0 8px 20px rgba(243, 156, 18, 0.2);
            position: relative;
            overflow: hidden;
        }

        .formula-box::before {
            content: '‚àÄŒµ>0';
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 1.2em;
            opacity: 0.3;
            color: #d35400;
            font-style: italic;
        }

        .example-box {
            background: linear-gradient(135deg, #e0c3fc 0%, #9bb5ff 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #9c88ff;
            box-shadow: 0 8px 20px rgba(156, 136, 255, 0.2);
            position: relative;
        }

        .example-box h4 {
            color: #5a4fcf;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .theorem-box {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(255, 107, 107, 0.3);
            position: relative;
        }

        .theorem-box::before {
            content: '‚àÉŒ¥>0';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 1.5em;
            opacity: 0.7;
            font-style: italic;
        }

        .theorem-box h4 {
            font-size: 1.3em;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        .highlight {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: bold;
            color: #2c3e50;
            display: inline-block;
            margin: 2px;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin: 20px 0;
        }

        .comparison-table th {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        .comparison-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }

        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .visual-demo {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            border: 3px solid #f39c12;
            text-align: center;
        }

        .bound-hierarchy {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 15px 0;
            font-size: 1.2em;
            font-weight: bold;
            flex-wrap: wrap;
        }

        .bound-hierarchy .arrow {
            color: #e67e22;
            font-size: 2em;
        }

        .bound-item {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
            color: white;
            padding: 15px 20px;
            border-radius: 15px;
            margin: 5px;
            min-width: 120px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(255, 107, 107, 0.3);
            transition: transform 0.3s ease;
        }

        .bound-item:hover {
            transform: scale(1.05);
        }

        .application-box {
            background: linear-gradient(135deg, #00b894 0%, #00a085 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(0, 184, 148, 0.3);
        }

        .application-box h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
            text-align: center;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 15px;
        }

        .pitfall-box {
            background: linear-gradient(135deg, #e17055 0%, #d63031 100%);
            color: white;
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            box-shadow: 0 8px 20px rgba(225, 112, 85, 0.3);
        }

        .pitfall-box h4 {
            font-size: 1.2em;
            margin-bottom: 10px;
        }

        ul, ol {
            margin-left: 25px;
            margin-top: 10px;
        }

        li {
            margin-bottom: 8px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
        }

        .nav-btn {
            padding: 12px 25px;
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 5px 15px rgba(255, 107, 107, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(255, 107, 107, 0.4);
        }

        @media (max-width: 768px) {
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .content {
                padding: 20px;
            }

            .bound-hierarchy {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chapter 5: Probability Inequalities</h1>
            <p>Essential Tools for Controlling Randomness and Uncertainty</p>
        </div>

        <div class="content">
            <div class="section">
                <h2>üéØ Why Study Probability Inequalities?</h2>
                <p>Probability inequalities are the <strong>backbone of statistical theory</strong>! They provide bounds on probabilities and expectations, enable concentration results, and form the theoretical foundation for statistical inference.</p>
                
                <div class="example-box">
                    <h4>üåü Real-World Motivation</h4>
                    <p><strong>Stock Market Example:</strong> You want to know how likely it is that your portfolio loses more than 20% in a month. Even if you don't know the exact distribution of returns, probability inequalities can give you useful bounds on this probability!</p>
                </div>

                <div class="visual-demo">
                    <h4>üîç The Big Picture</h4>
                    <p>Inequalities let us say things like:</p>
                    <div class="bound-hierarchy">
                        <span class="bound-item">"At least 75% of data"</span>
                        <span class="arrow">‚Üí</span>
                        <span class="bound-item">"within 2œÉ of mean"</span>
                        <span class="arrow">‚Üí</span>
                        <span class="bound-item">"regardless of distribution!"</span>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>‚öñÔ∏è Basic Inequalities - The Foundation</h2>

                <div class="inequality-type">
                    <h4>üìè Markov's Inequality</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        For X ‚â• 0 and a > 0:
                        <br><strong>P(X ‚â• a) ‚â§ E[X]/a</strong>
                    </div>
                    <p><strong>Key Insight:</strong> Uses only the mean! Very general but often loose.</p>
                    <p><strong>Proof Idea:</strong> E[X] = ‚à´ x dP ‚â• ‚à´_{X‚â•a} x dP ‚â• a¬∑P(X ‚â• a)</p>
                </div>

                <div class="inequality-type">
                    <h4>üìä Chebyshev's Inequality</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        P(|X - Œº| ‚â• k) ‚â§ œÉ¬≤/k¬≤
                        <br>P(|X - Œº| ‚â• kœÉ) ‚â§ 1/k¬≤
                    </div>
                    <p><strong>Famous Result:</strong> At least 75% of data within 2œÉ, 89% within 3œÉ</p>
                    <p><strong>Proof:</strong> Apply Markov to (X - Œº)¬≤</p>
                </div>

                <div class="example-box">
                    <h4>üé≤ Concrete Example: Dice Rolling</h4>
                    <p><strong>Setup:</strong> Roll a fair die, X = outcome</p>
                    <p><strong>Known:</strong> E[X] = 3.5, Var(X) = 35/12 ‚âà 2.92</p>
                    <p><strong>Markov:</strong> P(X ‚â• 5) ‚â§ 3.5/5 = 0.7 (actual = 2/6 ‚âà 0.33)</p>
                    <p><strong>Chebyshev:</strong> P(|X - 3.5| ‚â• 2) ‚â§ 2.92/4 = 0.73 (actual = 4/6 ‚âà 0.67)</p>
                </div>
            </div>

            <div class="section">
                <h2>üîó Union Bound - Controlling Multiple Events</h2>

                <div class="theorem-box">
                    <h4>üéØ Boole's Inequality</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        P(‚ãÉ·µ¢ A·µ¢) ‚â§ Œ£·µ¢ P(A·µ¢)
                    </div>
                    <p><strong>Interpretation:</strong> Probability of any event occurring ‚â§ sum of individual probabilities</p>
                </div>

                <div class="application-box">
                    <h4>üéØ Critical Applications</h4>
                    <ul>
                        <li><strong>Multiple Testing:</strong> Control family-wise error rate in statistical tests</li>
                        <li><strong>Machine Learning:</strong> Bound generalization error over hypothesis classes</li>
                        <li><strong>Network Reliability:</strong> Bound probability of any component failure</li>
                        <li><strong>Algorithm Analysis:</strong> Worst-case probability bounds</li>
                    </ul>
                </div>

                <div class="example-box">
                    <h4>üìö Multiple Testing Crisis</h4>
                    <p><strong>Problem:</strong> Testing 1000 hypotheses at Œ± = 0.05 each</p>
                    <p><strong>Naive approach:</strong> P(at least one false positive) could be huge!</p>
                    <div class="formula-box">
                        By Union Bound: P(any false positive) ‚â§ 1000 √ó 0.05 = 50!
                    </div>
                    <p><strong>Solution:</strong> Bonferroni correction: Use Œ±/1000 = 0.00005 for each test</p>
                </div>
            </div>

            <div class="section">
                <h2>üìà Chernoff Bounds - Exponential Concentration</h2>

                <div class="theorem-box">
                    <h4>üöÄ The Chernoff Method</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        P(X ‚â• a) ‚â§ inf_{t>0} M_X(t)e^{-ta}
                    </div>
                    <p><strong>Strategy:</strong> Use moment generating functions to get exponential bounds!</p>
                </div>

                <div class="grid-2">
                    <div class="inequality-type">
                        <h4>üéØ Hoeffding's Inequality</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            For X·µ¢ ‚àà [a·µ¢,b·µ¢] independent:
                            <br>P(|S‚Çô - E[S‚Çô]| ‚â• t) ‚â§ 2exp(-2t¬≤/Œ£(b·µ¢-a·µ¢)¬≤)
                        </div>
                        <p><strong>Key:</strong> Exponential decay in tail probability!</p>
                    </div>

                    <div class="inequality-type">
                        <h4>‚ö° Bernstein's Inequality</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            P(S‚Çô ‚â• t) ‚â§ exp(-t¬≤/(2(v + Mt/3)))
                        </div>
                        <p><strong>Advantage:</strong> Better when variance v is small!</p>
                    </div>
                </div>

                <div class="visual-demo">
                    <h4>üìä Exponential vs Polynomial Decay</h4>
                    <div class="bound-hierarchy">
                        <span class="bound-item">Markov: 1/t</span>
                        <span class="arrow">vs</span>
                        <span class="bound-item">Chebyshev: 1/t¬≤</span>
                        <span class="arrow">vs</span>
                        <span class="bound-item">Chernoff: e^{-ct¬≤}</span>
                    </div>
                    <p><strong>Exponential bounds are MUCH stronger!</strong></p>
                </div>
            </div>

            <div class="section">
                <h2>üéØ Concentration Inequalities - Modern Tools</h2>

                <div class="theorem-box">
                    <h4>üåü Sub-Gaussian Random Variables</h4>
                    <p>X is <strong>sub-Gaussian</strong> with parameter œÉ if:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        E[e^{t(X-E[X])}] ‚â§ e^{œÉ¬≤t¬≤/2}
                    </div>
                </div>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>üî¢ Examples of Sub-Gaussian</h4>
                        <ul>
                            <li><strong>Gaussian variables</strong></li>
                            <li><strong>Bounded variables:</strong> X ‚àà [a,b]</li>
                            <li><strong>Rademacher variables:</strong> P(X = ¬±1) = 1/2</li>
                        </ul>
                    </div>

                    <div class="inequality-type" style="background: linear-gradient(135deg, #00b894 0%, #00a085 100%);">
                        <h4>‚ö° Sub-Gaussian Concentration</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            P(|S‚Çô - E[S‚Çô]| ‚â• t) ‚â§ 2e^{-t¬≤/(2nœÉ¬≤)}
                        </div>
                    </div>

                    <div class="example-box">
                        <h4>üìà Sub-Exponential</h4>
                        <p><strong>Heavier tails than sub-Gaussian</strong></p>
                        <p>Examples: squared Gaussians, exponential variables</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üé≤ Martingale Inequalities - Sequential Processes</h2>

                <div class="theorem-box">
                    <h4>üåä Azuma's Inequality</h4>
                    <p>For martingale with bounded differences |M·µ¢ - M·µ¢‚Çã‚ÇÅ| ‚â§ c·µ¢:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        P(|M‚Çô - M‚ÇÄ| ‚â• t) ‚â§ 2exp(-t¬≤/(2Œ£c·µ¢¬≤))
                    </div>
                </div>

                <div class="grid-2">
                    <div class="inequality-type">
                        <h4>üîÑ McDiarmid's Inequality</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            For function f with bounded differences:
                            <br>P(|f(X‚ÇÅ,...,X‚Çô) - E[f]| ‚â• t) ‚â§ 2exp(-2t¬≤/Œ£c·µ¢¬≤)
                        </div>
                    </div>

                    <div class="application-box" style="background: linear-gradient(135deg, #6c5ce7 0%, #5f3dc4 100%);">
                        <h4>üöÄ Applications</h4>
                        <ul>
                            <li><strong>Random Graphs:</strong> Chromatic numbers</li>
                            <li><strong>Machine Learning:</strong> Generalization bounds</li>
                            <li><strong>Algorithms:</strong> Randomized analysis</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üî¢ Matrix Concentration - High-Dimensional Data</h2>

                <div class="theorem-box">
                    <h4>üßÆ Matrix Chernoff Bound</h4>
                    <p>For independent random matrices X·µ¢:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        P(Œª‚Çò‚Çê‚Çì(Œ£ X·µ¢ - E[Œ£ X·µ¢]) ‚â• t) ‚â§ d¬∑e^{-t¬≤/(2œÉ¬≤)}
                    </div>
                </div>

                <div class="application-box">
                    <h4>üéØ Modern Applications</h4>
                    <ul>
                        <li><strong>Random Matrix Theory:</strong> Eigenvalue bounds for random matrices</li>
                        <li><strong>Covariance Estimation:</strong> Sample covariance concentration</li>
                        <li><strong>Principal Component Analysis:</strong> Perturbation bounds</li>
                        <li><strong>Machine Learning:</strong> High-dimensional generalization</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>üèóÔ∏è Hierarchy of Bounds</h2>

                <div class="visual-demo">
                    <h4>üìä The Strength Hierarchy</h4>
                    <p><em>More assumptions ‚Üí Tighter bounds</em></p>
                    
                    <div class="bound-hierarchy">
                        <div class="bound-item">Markov<br><small>Mean only</small></div>
                        <div class="arrow">‚Üí</div>
                        <div class="bound-item">Chebyshev<br><small>+ Variance</small></div>
                        <div class="arrow">‚Üí</div>
                        <div class="bound-item">Chernoff<br><small>+ MGF</small></div>
                        <div class="arrow">‚Üí</div>
                        <div class="bound-item">Hoeffding<br><small>+ Bounded</small></div>
                    </div>
                </div>

                <table class="comparison-table">
                    <tr>
                        <th>Inequality</th>
                        <th>Assumptions</th>
                        <th>Bound Type</th>
                        <th>When to Use</th>
                    </tr>
                    <tr>
                        <td><strong>Markov</strong></td>
                        <td>X ‚â• 0, E[X] exists</td>
                        <td>1/t</td>
                        <td>Only mean known</td>
                    </tr>
                    <tr>
                        <td><strong>Chebyshev</strong></td>
                        <td>Finite variance</td>
                        <td>1/t¬≤</td>
                        <td>Mean and variance known</td>
                    </tr>
                    <tr>
                        <td><strong>Chernoff</strong></td>
                        <td>MGF exists</td>
                        <td>e^{-ct}</td>
                        <td>Want exponential bounds</td>
                    </tr>
                    <tr>
                        <td><strong>Hoeffding</strong></td>
                        <td>Bounded variables</td>
                        <td>e^{-ct¬≤}</td>
                        <td>Bounded independent variables</td>
                    </tr>
                </table>
            </div>

            <div class="section">
                <h2>üéØ Modern Applications in Statistics</h2>

                <div class="grid-3">
                    <div class="application-box">
                        <h4>ü§ñ Machine Learning</h4>
                        <ul>
                            <li><strong>PAC Learning:</strong> Sample complexity bounds</li>
                            <li><strong>Generalization:</strong> P(R(h) - RÃÇ(h) ‚â• Œµ) ‚â§ Œ¥</li>
                            <li><strong>Online Learning:</strong> Regret bounds</li>
                            <li><strong>High-dimensional:</strong> Sparse recovery theory</li>
                        </ul>
                    </div>

                    <div class="application-box">
                        <h4>üìä Hypothesis Testing</h4>
                        <ul>
                            <li><strong>Multiple Testing:</strong> FDR control</li>
                            <li><strong>Sequential Testing:</strong> Early stopping</li>
                            <li><strong>Chernoff bounds:</strong> Exponential error rates</li>
                            <li><strong>Minimax theory:</strong> Lower bounds via Fano's inequality</li>
                        </ul>
                    </div>

                    <div class="application-box">
                        <h4>üí∞ Risk Management</h4>
                        <ul>
                            <li><strong>VaR bounds:</strong> Value at risk calculations</li>
                            <li><strong>Tail risk:</strong> Extreme event probabilities</li>
                            <li><strong>Portfolio theory:</strong> Concentration bounds for returns</li>
                            <li><strong>Stress testing:</strong> Worst-case scenario bounds</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üî¨ Advanced Topics - Cutting Edge</h2>

                <div class="grid-2">
                    <div class="theorem-box">
                        <h4>üåä Empirical Process Theory</h4>
                        <p><strong>DKW Inequality:</strong></p>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            P(sup_x |FÃÇ‚Çô(x) - F(x)| > Œµ) ‚â§ 2e^{-2nŒµ¬≤}
                        </div>
                        <p><strong>Applications:</strong> Kolmogorov-Smirnov test, goodness-of-fit</p>
                    </div>

                    <div class="theorem-box">
                        <h4>üéØ Rademacher Complexity</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            R_n(‚Ñ±) = E[sup_{f‚àà‚Ñ±} (1/n)Œ£·µ¢œÉ·µ¢f(X·µ¢)]
                        </div>
                        <p><strong>Bound:</strong> E[sup_{f‚àà‚Ñ±} |P‚Çôf - Pf|] ‚â§ 2R_n(‚Ñ±)</p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üß† Information-Theoretic Inequalities</h4>
                    <ul>
                        <li><strong>Fano's Inequality:</strong> H(Œ∏|X) ‚â§ h_b(P_e) + P_e log(|Œò| - 1)</li>
                        <li><strong>Data Processing:</strong> I(X;Y) ‚â• I(X;Z) when X ‚Üí Y ‚Üí Z</li>
                        <li><strong>Applications:</strong> Lower bounds on estimation error</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>‚ö†Ô∏è Common Pitfalls and How to Avoid Them</h2>

                <div class="grid-2">
                    <div class="pitfall-box">
                        <h4>‚ùå Wrong tail direction</h4>
                        <p><strong>Problem:</strong> Confusing one-sided vs two-sided bounds</p>
                        <p><strong>Solution:</strong> Always specify which tail you're bounding</p>
                        <p><strong>Remember:</strong> P(|X| ‚â• t) ‚â§ 2P(X ‚â• t) for symmetric distributions</p>
                    </div>

                    <div class="pitfall-box">
                        <h4>‚ùå Independence assumptions</h4>
                        <p><strong>Problem:</strong> Many bounds require independence</p>
                        <p><strong>Reality:</strong> Real data often has dependence</p>
                        <p><strong>Solution:</strong> Use martingale or mixing inequalities</p>
                    </div>
                </div>

                <div class="pitfall-box">
                    <h4>‚ùå Ignoring constants in bounds</h4>
                    <p><strong>Problem:</strong> Inequality constants often not sharp for finite samples</p>
                    <p><strong>Example:</strong> Chebyshev bound is often very loose compared to actual probability</p>
                    <p><strong>Solution:</strong> Use bounds for qualitative insights, not precise quantitative predictions</p>
                </div>

                <div class="grid-2">
                    <div class="pitfall-box">
                        <h4>‚ùå Moment conditions</h4>
                        <p><strong>Problem:</strong> Forgetting to check if required moments exist</p>
                        <p><strong>Example:</strong> Chebyshev requires finite variance</p>
                        <p><strong>Solution:</strong> Always verify assumptions!</p>
                    </div>

                    <div class="pitfall-box">
                        <h4>‚ùå Misusing union bounds</h4>
                        <p><strong>Problem:</strong> Union bound can be very loose</p>
                        <p><strong>Better:</strong> Use Bonferroni-Holm or other refined methods</p>
                        <p><strong>When tight:</strong> When events are nearly disjoint</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üîó Connections to Other Chapters</h2>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>üìà To Convergence (Chapter 6)</h4>
                        <ul>
                            <li><strong>Law of Large Numbers:</strong> Chebyshev inequality proof</li>
                            <li><strong>Central Limit Theorem:</strong> Berry-Esseen bounds</li>
                            <li><strong>Concentration:</strong> Almost sure convergence</li>
                            <li><strong>Rates:</strong> How fast convergence happens</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üìä To CDF Estimation (Chapter 8)</h4>
                        <ul>
                            <li><strong>Glivenko-Cantelli:</strong> Uniform convergence</li>
                            <li><strong>DKW inequality:</strong> Concentration of empirical CDF</li>
                            <li><strong>Kolmogorov-Smirnov:</strong> Test statistics</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üî¨ To Hypothesis Testing (Chapter 11)</h4>
                        <ul>
                            <li><strong>Type I/II errors:</strong> Error probability bounds</li>
                            <li><strong>Multiple testing:</strong> Family-wise error control</li>
                            <li><strong>Sequential tests:</strong> Stopping time analysis</li>
                        </ul>
                    </div>
                </div>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>ü§ñ To Machine Learning (Chapter 23)</h4>
                        <ul>
                            <li><strong>Generalization bounds:</strong> PAC learning theory</li>
                            <li><strong>Empirical risk:</strong> Concentration inequalities</li>
                            <li><strong>High-dimensional:</strong> Sparse recovery bounds</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üìà To Regression (Chapters 14-15)</h4>
                        <ul>
                            <li><strong>Least squares:</strong> Concentration of estimators</li>
                            <li><strong>High-dimensional:</strong> LASSO theory</li>
                            <li><strong>Prediction:</strong> Generalization bounds</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>‚è∞ To Time Series (Chapter 18)</h4>
                        <ul>
                            <li><strong>Dependent data:</strong> Martingale inequalities</li>
                            <li><strong>Mixing sequences:</strong> Modified concentration</li>
                            <li><strong>Long memory:</strong> Modified bounds</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üåü Summary and Key Takeaways</h2>

                <div class="application-box">
                    <h4>üéØ Main Messages</h4>
                    <ul>
                        <li><strong>Hierarchy of bounds:</strong> More assumptions ‚Üí tighter bounds</li>
                        <li><strong>Exponential vs polynomial:</strong> Chernoff bounds much stronger than Chebyshev</li>
                        <li><strong>Independence matters:</strong> Most results require careful checking</li>
                        <li><strong>Modern applications:</strong> Essential for machine learning and high-dimensional statistics</li>
                        <li><strong>Practical tool:</strong> Guide intuition even when not numerically tight</li>
                    </ul>
                </div>

                <div class="visual-demo">
                    <h4>üöÄ The Big Picture</h4>
                    <p>Probability inequalities are the <strong>mathematical foundation</strong> that lets us:</p>
                    <ul style="text-align: left; max-width: 600px; margin: 0 auto;">
                        <li>Bound the probability of rare events</li>
                        <li>Control error rates in statistical procedures</li>
                        <li>Prove convergence theorems</li>
                        <li>Design algorithms with performance guarantees</li>
                        <li>Understand when statistical methods work</li>
                    </ul>
                </div>

                <div class="theorem-box">
                    <h4>üéì Practical Guidelines</h4>
                    <ul>
                        <li><strong>Start simple:</strong> Try Markov/Chebyshev first</li>
                        <li><strong>Check assumptions:</strong> Independence, moment conditions</li>
                        <li><strong>Choose the right tool:</strong> Match inequality to problem structure</li>
                        <li><strong>Don't trust constants:</strong> Use for qualitative insights</li>
                        <li><strong>Combine methods:</strong> Union bounds + concentration often powerful</li>
                    </ul>
                </div>

                <div class="example-box">
                    <h4>üîÆ Looking Forward</h4>
                    <p><strong>Modern developments:</strong></p>
                    <ul>
                        <li><strong>High-dimensional probability:</strong> Dimension-free bounds</li>
                        <li><strong>Non-commutative inequalities:</strong> Matrix concentration</li>
                        <li><strong>Optimal transport:</strong> Wasserstein inequalities</li>
                        <li><strong>Machine learning theory:</strong> Algorithm-specific bounds</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="#" class="nav-btn">‚Üê Chapter 4: Expectation</a>
            <a href="#" class="nav-btn">Chapter 6: Convergence ‚Üí</a>
        </div>
    </div>
</body>
</html>