<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 15: Multiple Regression</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 50%, #ffecd2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: 'üìàüìäüîóüìâüìà';
            position: absolute;
            top: 50%;
            left: -20%;
            transform: translateY(-50%);
            font-size: 4em;
            opacity: 0.1;
            animation: regression-flow 16s linear infinite;
        }

        @keyframes regression-flow {
            0% { left: -20%; }
            100% { left: 120%; }
        }

        .header h1 {
            position: relative;
            z-index: 1;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header p {
            position: relative;
            z-index: 1;
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 40px;
            padding: 30px;
            border-radius: 15px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            border-left: 5px solid #667eea;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .section::before {
            content: '';
            position: absolute;
            top: -50px;
            right: -50px;
            width: 150px;
            height: 150px;
            background: radial-gradient(circle, rgba(102,126,234,0.1) 0%, transparent 70%);
            border-radius: 50%;
            animation: data-flow 8s ease-in-out infinite;
        }

        @keyframes data-flow {
            0%, 100% { transform: translate(0, 0) scale(1); }
            50% { transform: translate(-30px, 30px) scale(1.2); }
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.2);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            display: inline-block;
            position: relative;
            z-index: 1;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            position: relative;
            z-index: 1;
        }

        .regression-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.3);
            position: relative;
            text-align: center;
        }

        .regression-box::before {
            content: 'Œ≤';
            position: absolute;
            top: 15px;
            right: 25px;
            font-size: 3em;
            opacity: 0.3;
            font-style: italic;
        }

        .regression-box h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 15px;
        }

        .formula-box {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #f39c12;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            box-shadow: 0 8px 20px rgba(243, 156, 18, 0.2);
            position: relative;
            overflow: hidden;
        }

        .formula-box::before {
            content: 'X‚Ä≤X';
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 2em;
            opacity: 0.2;
            color: #d35400;
            font-style: italic;
        }

        .example-box {
            background: linear-gradient(135deg, #e0c3fc 0%, #9bb5ff 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #9c88ff;
            box-shadow: 0 8px 20px rgba(156, 136, 255, 0.2);
            position: relative;
        }

        .example-box h4 {
            color: #5a4fcf;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .assumption-box {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(255, 107, 107, 0.3);
            position: relative;
        }

        .assumption-box::before {
            content: '‚ö†Ô∏è';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 2em;
            opacity: 0.7;
        }

        .assumption-box h4 {
            font-size: 1.3em;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        .highlight {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: bold;
            color: #2c3e50;
            display: inline-block;
            margin: 2px;
        }

        .method-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .method-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.3);
            transition: all 0.3s ease;
        }

        .method-card:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.4);
        }

        .method-card h5 {
            font-size: 1.3em;
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }

        .diagnostic-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin: 20px 0;
        }

        .comparison-table th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        .comparison-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }

        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .interpretation-demo {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            border: 3px solid #26a69a;
            text-align: center;
        }

        ul, ol {
            margin-left: 25px;
            margin-top: 10px;
        }

        li {
            margin-bottom: 8px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        }

        .nav-btn {
            padding: 12px 25px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.4);
        }

        @media (max-width: 768px) {
            .method-grid, .diagnostic-grid {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .content {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chapter 15: Multiple Regression</h1>
            <p>Beyond Simple Linear Relationships: Modeling Complex Dependencies</p>
        </div>

        <div class="content">
            <div class="section">
                <h2>üéØ Introduction to Multiple Regression</h2>
                <p>Multiple regression extends simple linear regression to include multiple predictor variables. This allows us to model complex relationships and control for confounding variables.</p>
                
                <div class="regression-box">
                    <h4>üìä The Multiple Linear Regression Model</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + ... + Œ≤‚ÇöX‚Çö + Œµ
                        <br><br>
                        <strong>Matrix form:</strong> Y = XŒ≤ + Œµ
                        <br>where Y is n√ó1, X is n√ó(p+1), Œ≤ is (p+1)√ó1, Œµ is n√ó1
                    </div>
                    <p><strong>Key advantage:</strong> Control for multiple variables simultaneously!</p>
                </div>

                <div class="example-box">
                    <h4>üè† Real Estate Example</h4>
                    <p><strong>Question:</strong> What determines house prices?</p>
                    <div class="formula-box">
                        Price = Œ≤‚ÇÄ + Œ≤‚ÇÅ√óSize + Œ≤‚ÇÇ√óBedrooms + Œ≤‚ÇÉ√óAge + Œ≤‚ÇÑ√óLocation + Œµ
                    </div>
                    <p><strong>Œ≤‚ÇÅ interpretation:</strong> Expected change in price for 1 sq ft increase, holding bedrooms, age, and location constant</p>
                    <p><strong>Key insight:</strong> "Holding other variables constant" (ceteris paribus) is crucial for causal interpretation!</p>
                </div>

                <div class="interpretation-demo">
                    <h4>üéØ Why Multiple Regression?</h4>
                    <p><strong>Single regression problem:</strong> Omitted variable bias</p>
                    <div class="formula-box">
                        <strong>Simple regression:</strong> Price = Œ±‚ÇÄ + Œ±‚ÇÅ√óSize + error
                        <br><strong>Problem:</strong> Œ±‚ÇÅ captures both direct effect of size AND correlation with omitted variables
                        <br><strong>Solution:</strong> Include relevant variables to get unbiased estimates
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üìê Least Squares Estimation</h2>

                <div class="regression-box">
                    <h4>üéØ Normal Equations</h4>
                    <p>Minimize sum of squared residuals: RSS = (Y - XŒ≤)'(Y - XŒ≤)</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        <strong>Solution:</strong> Œ≤ÃÇ = (X'X)‚Åª¬πX'Y
                        <br><br>
                        <strong>Fitted values:</strong> ≈∂ = XŒ≤ÃÇ = X(X'X)‚Åª¬πX'Y = HY
                        <br><strong>Residuals:</strong> √™ = Y - ≈∂ = (I - H)Y
                        <br><strong>Hat matrix:</strong> H = X(X'X)‚Åª¬πX'
                    </div>
                    <p><strong>H projects Y onto column space of X</strong></p>
                </div>

                <h3>Geometric Interpretation</h3>
                <div class="example-box">
                    <h4>üìê Projection in n-Dimensional Space</h4>
                    <p><strong>Key insight:</strong> ≈∂ is the orthogonal projection of Y onto the column space of X</p>
                    <ul>
                        <li><strong>Column space of X:</strong> All possible linear combinations of columns</li>
                        <li><strong>Fitted values ≈∂:</strong> Point in column space closest to Y</li>
                        <li><strong>Residuals √™:</strong> Component of Y orthogonal to column space</li>
                        <li><strong>Pythagoras:</strong> ||Y||¬≤ = ||≈∂||¬≤ + ||√™||¬≤</li>
                    </ul>
                </div>

                <div class="method-grid">
                    <div class="method-card">
                        <h5>üìä Properties of OLS Estimator</h5>
                        <ul>
                            <li><strong>Unbiased:</strong> E[Œ≤ÃÇ] = Œ≤ (under assumptions)</li>
                            <li><strong>BLUE:</strong> Best Linear Unbiased Estimator (Gauss-Markov)</li>
                            <li><strong>Consistent:</strong> Œ≤ÃÇ ‚Üí·µñ Œ≤ as n ‚Üí ‚àû</li>
                            <li><strong>Asymptotically normal:</strong> ‚àön(Œ≤ÃÇ - Œ≤) ‚áí N(0, œÉ¬≤(X'X/n)‚Åª¬π)</li>
                        </ul>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Var(Œ≤ÃÇ) = œÉ¬≤(X'X)‚Åª¬π
                        </div>
                    </div>

                    <div class="method-card">
                        <h5>üìà R-squared and Model Fit</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            R¬≤ = 1 - SSE/TSS = SSR/TSS
                            <br><br>
                            TSS = Œ£(Y·µ¢ - »≤)¬≤ (Total Sum of Squares)
                            <br>SSE = Œ£(Y·µ¢ - ≈∂·µ¢)¬≤ (Sum Squared Errors)
                            <br>SSR = Œ£(≈∂·µ¢ - »≤)¬≤ (Sum Squares Regression)
                        </div>
                        <p><strong>Interpretation:</strong> Proportion of variance explained by the model</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>‚öñÔ∏è Assumptions of Linear Regression</h2>

                <div class="method-grid">
                    <div class="assumption-box">
                        <h4>1Ô∏è‚É£ Linearity</h4>
                        <p><strong>Assumption:</strong> E[Y|X] = XŒ≤</p>
                        <p><strong>What it means:</strong> True relationship is linear in parameters</p>
                        <p><strong>Violation:</strong> Systematic patterns in residuals</p>
                        <p><strong>Fix:</strong> Transform variables, add polynomial terms, use non-linear models</p>
                    </div>

                    <div class="assumption-box">
                        <h4>2Ô∏è‚É£ Independence</h4>
                        <p><strong>Assumption:</strong> Observations are independent</p>
                        <p><strong>What it means:</strong> Cov(Œµ·µ¢, Œµ‚±º) = 0 for i ‚â† j</p>
                        <p><strong>Violation:</strong> Time series, spatial, or clustered data</p>
                        <p><strong>Fix:</strong> Use robust standard errors, time series methods, mixed effects models</p>
                    </div>

                    <div class="assumption-box">
                        <h4>3Ô∏è‚É£ Homoscedasticity</h4>
                        <p><strong>Assumption:</strong> Constant variance Var(Œµ|X) = œÉ¬≤</p>
                        <p><strong>What it means:</strong> Error variance doesn't depend on X</p>
                        <p><strong>Violation:</strong> Heteroscedasticity (fan-shaped residuals)</p>
                        <p><strong>Fix:</strong> Transform Y, weighted least squares, robust SE</p>
                    </div>

                    <div class="assumption-box">
                        <h4>4Ô∏è‚É£ Normality</h4>
                        <p><strong>Assumption:</strong> Œµ ~ N(0, œÉ¬≤I)</p>
                        <p><strong>What it means:</strong> Errors are normally distributed</p>
                        <p><strong>Note:</strong> Only needed for exact inference in small samples</p>
                        <p><strong>Robust:</strong> CLT ensures asymptotic normality</p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üîç Gauss-Markov Theorem</h4>
                    <p><strong>Under assumptions 1-3:</strong> OLS is BLUE (Best Linear Unbiased Estimator)</p>
                    <p><strong>Best:</strong> Among all linear unbiased estimators, OLS has minimum variance</p>
                    <p><strong>Does NOT require normality!</strong> Normality only needed for distributional results</p>
                </div>
            </div>

            <div class="section">
                <h2>üß™ Hypothesis Testing</h2>

                <h3>Testing Individual Coefficients</h3>
                <div class="regression-box">
                    <h4>üéØ t-tests for Œ≤‚±º</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        H‚ÇÄ: Œ≤‚±º = 0 vs H‚ÇÅ: Œ≤‚±º ‚â† 0
                        <br><br>
                        t = Œ≤ÃÇ‚±º/SE(Œ≤ÃÇ‚±º) ~ t_{n-p-1}
                        <br><br>
                        SE(Œ≤ÃÇ‚±º) = œÉÃÇ‚àö[(X'X)‚Åª¬π]‚±º‚±º where œÉÃÇ¬≤ = SSE/(n-p-1)
                    </div>
                    <p><strong>Interpretation:</strong> Is X‚±º significantly related to Y after controlling for other variables?</p>
                </div>

                <h3>Testing Multiple Coefficients</h3>
                <div class="regression-box">
                    <h4>üéØ F-tests for Model Significance</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        H‚ÇÄ: Œ≤‚ÇÅ = Œ≤‚ÇÇ = ... = Œ≤‚Çö = 0 (no relationship)
                        <br>H‚ÇÅ: At least one Œ≤‚±º ‚â† 0
                        <br><br>
                        F = (SSR/p)/(SSE/(n-p-1)) = (R¬≤/p)/((1-R¬≤)/(n-p-1))
                        <br><br>
                        F ~ F_{p,n-p-1} under H‚ÇÄ
                    </div>
                    <p><strong>Tests overall model significance</strong></p>
                </div>

                <div class="example-box">
                    <h4>üìä Partial F-tests</h4>
                    <p><strong>Compare nested models:</strong> Test if subset of coefficients equals zero</p>
                    <div class="formula-box">
                        F = ((SSE_R - SSE_F)/q)/(SSE_F/(n-p-1))
                        <br><br>
                        SSE_R: from restricted model (H‚ÇÄ true)
                        <br>SSE_F: from full model
                        <br>q: number of restrictions
                    </div>
                    <p><strong>Example:</strong> Test if Œ≤‚ÇÇ = Œ≤‚ÇÉ = 0 simultaneously</p>
                </div>
            </div>

            <div class="section">
                <h2>üîç Diagnostics and Model Checking</h2>

                <div class="diagnostic-grid">
                    <div class="method-card">
                        <h5>üìà Residual Plots</h5>
                        <p><strong>Residuals vs Fitted:</strong> Check linearity and homoscedasticity</p>
                        <p><strong>Good:</strong> Random cloud around zero</p>
                        <p><strong>Bad:</strong> Patterns, trends, or fan shapes</p>
                        <p><strong>Normal Q-Q:</strong> Check normality assumption</p>
                        <p><strong>Scale-Location:</strong> Check homoscedasticity</p>
                    </div>

                    <div class="method-card">
                        <h5>üéØ Leverage and Influence</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            h·µ¢·µ¢ = X'·µ¢(X'X)‚Åª¬πX·µ¢ (leverage)
                            <br>High leverage: h·µ¢·µ¢ > 2(p+1)/n
                        </div>
                        <p><strong>Cook's Distance:</strong> Overall influence</p>
                        <p><strong>DFBETAS:</strong> Influence on specific coefficients</p>
                    </div>

                    <div class="method-card">
                        <h5>‚ö†Ô∏è Outlier Detection</h5>
                        <p><strong>Standardized residuals:</strong> r·µ¢/‚àö(MSE(1-h·µ¢·µ¢))</p>
                        <p><strong>Studentized residuals:</strong> Leave-one-out standard errors</p>
                        <p><strong>Rule of thumb:</strong> |standardized residual| > 2 suspicious</p>
                        <p><strong>Bonferroni correction:</strong> Multiple testing adjustment</p>
                    </div>

                    <div class="method-card">
                        <h5>üîó Multicollinearity</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            VIF_j = 1/(1 - R¬≤_j)
                        </div>
                        <p><strong>VIF > 10:</strong> Serious multicollinearity</p>
                        <p><strong>Problems:</strong> Unstable coefficients, large SE</p>
                        <p><strong>Solutions:</strong> Remove variables, ridge regression, PCA</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üéØ Model Selection</h2>

                <div class="method-grid">
                    <div class="method-card">
                        <h5>üìè Information Criteria</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            AIC = n√ólog(SSE/n) + 2(p+1)
                            <br>BIC = n√ólog(SSE/n) + (p+1)√ólog(n)
                            <br>Adjusted R¬≤ = 1 - (1-R¬≤)(n-1)/(n-p-1)
                        </div>
                        <p><strong>Lower AIC/BIC is better</strong></p>
                        <p><strong>Higher Adjusted R¬≤ is better</strong></p>
                        <p><strong>BIC penalizes complexity more heavily</strong></p>
                    </div>

                    <div class="method-card">
                        <h5>üîÑ Stepwise Selection</h5>
                        <p><strong>Forward:</strong> Start empty, add variables</p>
                        <p><strong>Backward:</strong> Start full, remove variables</p>
                        <p><strong>Stepwise:</strong> Can add or remove at each step</p>
                        <p><strong>Criteria:</strong> AIC, BIC, p-values</p>
                        <p><strong>Warning:</strong> Can overfit, multiple testing issues</p>
                    </div>

                    <div class="method-card">
                        <h5>‚úÖ Cross-Validation</h5>
                        <p><strong>k-fold CV:</strong> Split data, train on k-1 folds, test on 1</p>
                        <p><strong>LOOCV:</strong> Leave-one-out cross-validation</p>
                        <p><strong>Advantage:</strong> Direct assessment of prediction accuracy</p>
                        <p><strong>Use:</strong> Compare models, tune hyperparameters</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üõ°Ô∏è Regularization Methods</h2>

                <div class="method-grid">
                    <div class="method-card">
                        <h5>üìè Ridge Regression</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Œ≤ÃÇ^ridge = (X'X + ŒªI)‚Åª¬πX'Y
                            <br><br>
                            Minimize: ||Y - XŒ≤||¬≤ + Œª||Œ≤||¬≤
                        </div>
                        <p><strong>Effect:</strong> Shrinks coefficients toward zero</p>
                        <p><strong>Advantage:</strong> Handles multicollinearity</p>
                        <p><strong>Disadvantage:</strong> Doesn't select variables</p>
                    </div>

                    <div class="method-card">
                        <h5>üìê Lasso Regression</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Minimize: ||Y - XŒ≤||¬≤ + Œª||Œ≤||‚ÇÅ
                            <br><br>
                            where ||Œ≤||‚ÇÅ = Œ£|Œ≤‚±º|
                        </div>
                        <p><strong>Effect:</strong> Sets some coefficients exactly to zero</p>
                        <p><strong>Advantage:</strong> Automatic variable selection</p>
                        <p><strong>Disadvantage:</strong> Can be unstable with correlated predictors</p>
                    </div>

                    <div class="method-card">
                        <h5>üéØ Elastic Net</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Minimize: ||Y - XŒ≤||¬≤ + Œª‚ÇÅ||Œ≤||‚ÇÅ + Œª‚ÇÇ||Œ≤||¬≤
                        </div>
                        <p><strong>Combines Ridge and Lasso penalties</strong></p>
                        <p><strong>Advantage:</strong> Variable selection + handles correlation</p>
                        <p><strong>Tuning:</strong> Choose Œª‚ÇÅ and Œª‚ÇÇ via cross-validation</p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üéØ When to Use Regularization</h4>
                    <p><strong>High-dimensional data:</strong> p approaches or exceeds n</p>
                    <p><strong>Multicollinearity:</strong> Predictors are highly correlated</p>
                    <p><strong>Overfitting:</strong> Model performs well on training but poorly on test data</p>
                    <p><strong>Prediction focus:</strong> Care more about prediction than interpretation</p>
                    
                    <table class="comparison-table">
                        <tr>
                            <th>Method</th>
                            <th>When to Use</th>
                            <th>Pros</th>
                            <th>Cons</th>
                        </tr>
                        <tr>
                            <td><strong>OLS</strong></td>
                            <td>n >> p, low correlation</td>
                            <td>Unbiased, interpretable</td>
                            <td>Can overfit, multicollinearity issues</td>
                        </tr>
                        <tr>
                            <td><strong>Ridge</strong></td>
                            <td>Multicollinearity, all variables relevant</td>
                            <td>Stable, handles correlation</td>
                            <td>No variable selection</td>
                        </tr>
                        <tr>
                            <td><strong>Lasso</strong></td>
                            <td>Sparse models, variable selection</td>
                            <td>Automatic selection</td>
                            <td>Arbitrary selection with correlation</td>
                        </tr>
                        <tr>
                            <td><strong>Elastic Net</strong></td>
                            <td>Best of both worlds</td>
                            <td>Selection + stability</td>
                            <td>Two tuning parameters</td>
                        </tr>
                    </table>
                </div>
            </div>

            <div class="section">
                <h2>‚ö†Ô∏è Common Pitfalls</h2>

                <div class="method-grid">
                    <div class="assumption-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);">
                        <h4>‚ùå Overfitting</h4>
                        <p><strong>Problem:</strong> Including too many variables relative to sample size</p>
                        <p><strong>Result:</strong> High R¬≤ but poor prediction on new data</p>
                        <p><strong>Solutions:</strong> Cross-validation, regularization, larger sample</p>
                        <p><strong>Rule of thumb:</strong> Need at least 10-15 observations per predictor</p>
                    </div>

                    <div class="assumption-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);">
                        <h4>‚ùå Data Snooping</h4>
                        <p><strong>Problem:</strong> Using same data for selection and inference</p>
                        <p><strong>Result:</strong> Inflated significance, overconfident inference</p>
                        <p><strong>Solutions:</strong> Split data, post-selection inference methods</p>
                        <p><strong>Remember:</strong> p-values invalid after variable selection!</p>
                    </div>

                    <div class="assumption-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);">
                        <h4>‚ùå Multicollinearity Ignored</h4>
                        <p><strong>Problem:</strong> Predictors highly correlated</p>
                        <p><strong>Result:</strong> Unstable coefficients, large standard errors</p>
                        <p><strong>Detection:</strong> High VIF, condition number</p>
                        <p><strong>Solutions:</strong> Remove variables, ridge regression, PCA</p>
                    </div>

                    <div class="assumption-box" style="background: linear-gradient(135deg, #ff6b6b 0, #ee5a6f 100%);">
                        <h4>‚ùå Assumption Violations</h4>
                        <p><strong>Problem:</strong> Not checking model assumptions</p>
                        <p><strong>Consequences:</strong> Biased estimates, invalid inference</p>
                        <p><strong>Always check:</strong> Residual plots, normality, independence</p>
                        <p><strong>Fixes:</strong> Transforms, robust methods, different models</p>
                    </div>

                    <div class="assumption-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);">
                        <h4>‚ùå Causal Interpretation</h4>
                        <p><strong>Problem:</strong> Confusing association with causation</p>
                        <p><strong>Reality:</strong> Regression shows conditional associations</p>
                        <p><strong>For causation:</strong> Need experimental data or strong assumptions</p>
                        <p><strong>Confounding:</strong> Omitted variables can bias results</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üéØ Practical Guidelines</h2>

                <div class="interpretation-demo">
                    <h4>üìã Model Building Workflow</h4>
                    <ol>
                        <li><strong>Exploratory Analysis:</strong> Visualize relationships, check for outliers</li>
                        <li><strong>Initial Model:</strong> Include theoretically important variables</li>
                        <li><strong>Diagnostic Checking:</strong> Residual plots, assumption tests</li>
                        <li><strong>Model Refinement:</strong> Transform variables, handle outliers</li>
                        <li><strong>Variable Selection:</strong> Use principled methods (CV, IC)</li>
                        <li><strong>Final Validation:</strong> Test on hold-out data</li>
                        <li><strong>Interpretation:</strong> Focus on practically significant effects</li>
                    </ol>
                </div>

                <div class="example-box">
                    <h4>üéØ Best Practices</h4>
                    <p><strong>Start simple:</strong> Begin with main effects before interactions</p>
                    <p><strong>Cross-validate rigorously:</strong> Use proper validation procedures</p>
                    <p><strong>Check assumptions:</strong> Don't trust results without diagnostics</p>
                    <p><strong>Report uncertainty:</strong> Include confidence intervals, not just p-values</p>
                    <p><strong>Focus on effect sizes:</strong> Statistical significance ‚â† practical importance</p>
                    <p><strong>Consider domain knowledge:</strong> Statistics can't replace subject expertise</p>
                </div>

                <div class="regression-box">
                    <h4>üåü Key Insights</h4>
                    <ul>
                        <li><strong>Flexibility vs Interpretability:</strong> More complex models may predict better but are harder to interpret</li>
                        <li><strong>Bias-Variance Tradeoff:</strong> Regularization trades some bias for reduced variance</li>
                        <li><strong>No Free Lunch:</strong> All methods make assumptions - understand and check them</li>
                        <li><strong>Context Matters:</strong> Choose methods based on goals (prediction vs inference)</li>
                        <li><strong>Validation is Crucial:</strong> Must assess out-of-sample performance</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>üîó Connections to Other Chapters</h2>

                <div class="method-grid">
                    <div class="example-box">
                        <h5>üìä To Simple Linear Regression</h5>
                        <p><strong>Extension:</strong> Multiple predictors instead of one</p>
                        <p><strong>Matrix formulation:</strong> Generalizes scalar formulas</p>
                        <p><strong>Similar diagnostics:</strong> Same assumption checking methods</p>
                    </div>

                    <div class="example-box">
                        <h5>üìà To High-Dimensional Statistics</h5>
                        <p><strong>Regularization methods:</strong> Essential when p ‚âà n or p > n</p>
                        <p><strong>Sparsity assumptions:</strong> Many coefficients are zero</p>
                        <p><strong>Oracle inequalities:</strong> Theoretical guarantees for regularized methods</p>
                    </div>

                    <div class="example-box">
                        <h5>ü§ñ To Machine Learning</h5>
                        <p><strong>Supervised learning:</strong> Regression is fundamental ML task</p>
                        <p><strong>Bias-variance tradeoff:</strong> Central concept in ML</p>
                        <p><strong>Cross-validation:</strong> Standard model selection technique</p>
                    </div>

                    <div class="example-box">
                        <h5>üéØ To Causal Inference</h5>
                        <p><strong>Confounding control:</strong> Including covariates to isolate causal effects</p>
                        <p><strong>Instrumental variables:</strong> Special case of regression for causal identification</p>
                        <p><strong>Mediation analysis:</strong> Understanding causal pathways</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="#" class="nav-btn">‚Üê Chapter 14: Simple Linear Regression</a>
            <a href="#" class="nav-btn">Chapter 16: Advanced Regression Topics ‚Üí</a>
        </div>
    </div>
</body>
</html>