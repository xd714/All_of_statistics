<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 22: Machine Learning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #667eea 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '🤖🧠🎯📊🔮';
            position: absolute;
            top: 50%;
            left: -25%;
            transform: translateY(-50%);
            font-size: 4em;
            opacity: 0.1;
            animation: ai-flow 18s linear infinite;
        }

        @keyframes ai-flow {
            0% { left: -25%; transform: translateY(-50%) rotate(0deg); }
            100% { left: 125%; transform: translateY(-50%) rotate(360deg); }
        }

        .header h1 {
            position: relative;
            z-index: 1;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header p {
            position: relative;
            z-index: 1;
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 40px;
            padding: 30px;
            border-radius: 15px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            border-left: 5px solid #667eea;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .section::before {
            content: '';
            position: absolute;
            top: -100px;
            right: -100px;
            width: 200px;
            height: 200px;
            background: radial-gradient(circle, rgba(102,126,234,0.1) 0%, transparent 70%);
            border-radius: 50%;
            animation: neural-pulse 6s ease-in-out infinite;
        }

        @keyframes neural-pulse {
            0%, 100% { transform: scale(1) rotate(0deg); opacity: 0.3; }
            50% { transform: scale(1.5) rotate(180deg); opacity: 0.1; }
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.2);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            display: inline-block;
            position: relative;
            z-index: 1;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            position: relative;
            z-index: 1;
        }

        .algorithm-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.3);
            position: relative;
            text-align: center;
        }

        .algorithm-box::before {
            content: '🧠';
            position: absolute;
            top: 15px;
            right: 25px;
            font-size: 3em;
            opacity: 0.3;
        }

        .algorithm-box h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 15px;
        }

        .formula-box {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #f39c12;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            box-shadow: 0 8px 20px rgba(243, 156, 18, 0.2);
            position: relative;
            overflow: hidden;
        }

        .formula-box::before {
            content: '∂/∂w';
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 2em;
            opacity: 0.2;
            color: #d35400;
            font-style: italic;
        }

        .example-box {
            background: linear-gradient(135deg, #e0c3fc 0%, #9bb5ff 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #9c88ff;
            box-shadow: 0 8px 20px rgba(156, 136, 255, 0.2);
            position: relative;
        }

        .example-box h4 {
            color: #5a4fcf;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .theory-box {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(255, 107, 107, 0.3);
            position: relative;
        }

        .theory-box::before {
            content: '∞';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 2.5em;
            opacity: 0.7;
            font-style: italic;
        }

        .theory-box h4 {
            font-size: 1.3em;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        .highlight {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: bold;
            color: #2c3e50;
            display: inline-block;
            margin: 2px;
        }

        .ml-methods {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .method-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.3);
            transition: all 0.3s ease;
            position: relative;
        }

        .method-card:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.4);
        }

        .method-card h5 {
            font-size: 1.3em;
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }

        .tradeoff-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin: 20px 0;
        }

        .tradeoff-table th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        .tradeoff-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }

        .tradeoff-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .bias-variance {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .complexity-demo {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            border: 3px solid #26a69a;
            text-align: center;
        }

        .regularization-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .neural-layers {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            text-align: center;
        }

        .layer-demo {
            background: rgba(255,255,255,0.2);
            margin: 10px 0;
            padding: 15px;
            border-radius: 10px;
            border-left: 4px solid rgba(255,255,255,0.5);
            font-family: monospace;
        }

        ul, ol {
            margin-left: 25px;
            margin-top: 10px;
        }

        li {
            margin-bottom: 8px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        }

        .nav-btn {
            padding: 12px 25px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.4);
        }

        @media (max-width: 768px) {
            .ml-methods, .bias-variance, .regularization-grid {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .content {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chapter 22: Machine Learning</h1>
            <p>Statistical Learning Theory Meets Modern AI</p>
        </div>

        <div class="content">
            <div class="section">
                <h2>🤖 What is Statistical Learning?</h2>
                <p>Machine learning is fundamentally about finding patterns in data to make predictions. It's the intersection of statistics, computer science, and optimization - using statistical principles to build algorithms that learn from experience.</p>
                
                <div class="example-box">
                    <h4>🌟 The Learning Problem</h4>
                    <p><strong>Given:</strong> Training data (X₁,Y₁), (X₂,Y₂), ..., (Xₙ,Yₙ)</p>
                    <p><strong>Goal:</strong> Find function f such that f(X) ≈ Y for new data</p>
                    <p><strong>Challenge:</strong> Generalize beyond the training set!</p>
                    
                    <div class="formula-box">
                        <strong>Key insight:</strong> We want to minimize expected prediction error:
                        <br>E[(Y - f(X))²]
                        <br>But we only observe training error:
                        <br>(1/n)∑(Yᵢ - f(Xᵢ))²
                    </div>
                </div>

                <div class="algorithm-box">
                    <h4>🎯 Core Machine Learning Tasks</h4>
                    <ul>
                        <li><strong>Supervised Learning:</strong> Predict Y from X using labeled data</li>
                        <li><strong>Unsupervised Learning:</strong> Find patterns in X without labels</li>
                        <li><strong>Reinforcement Learning:</strong> Learn optimal actions through trial and error</li>
                        <li><strong>Semi-supervised:</strong> Combine labeled and unlabeled data</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>⚖️ The Bias-Variance Tradeoff</h2>

                <div class="theory-box">
                    <h4>📊 Fundamental Decomposition</h4>
                    <p>For any estimator f̂, the expected prediction error decomposes as:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        E[(Y - f̂(X))²] = σ² + Bias²[f̂(X)] + Var[f̂(X)]
                    </div>
                    <ul>
                        <li><strong>σ²:</strong> Irreducible error (noise in Y)</li>
                        <li><strong>Bias²:</strong> Error from wrong assumptions</li>
                        <li><strong>Variance:</strong> Error from sensitivity to training data</li>
                    </ul>
                </div>

                <div class="bias-variance">
                    <div class="method-card">
                        <h5>🎯 High Bias, Low Variance</h5>
                        <p><strong>Characteristics:</strong></p>
                        <ul>
                            <li>Simple models</li>
                            <li>Strong assumptions</li>
                            <li>Consistent but wrong</li>
                            <li>Underfitting</li>
                        </ul>
                        <p><strong>Examples:</strong> Linear regression, linear SVM</p>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Training error ≈ Test error
                            <br>Both relatively high
                        </div>
                    </div>

                    <div class="method-card">
                        <h5>🎲 Low Bias, High Variance</h5>
                        <p><strong>Characteristics:</strong></p>
                        <ul>
                            <li>Complex models</li>
                            <li>Flexible assumptions</li>
                            <li>Changes a lot with data</li>
                            <li>Overfitting</li>
                        </ul>
                        <p><strong>Examples:</strong> Deep neural networks, high-degree polynomials</p>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Training error << Test error
                            <br>Large generalization gap
                        </div>
                    </div>
                </div>

                <div class="complexity-demo">
                    <h4>🎯 Model Complexity Sweet Spot</h4>
                    <div class="formula-box">
                        <strong>Too simple:</strong> High bias → Underfitting
                        <br><strong>Too complex:</strong> High variance → Overfitting
                        <br><strong>Just right:</strong> Balanced bias-variance → Good generalization
                    </div>
                    <p><strong>Goal:</strong> Find the complexity that minimizes total error!</p>
                </div>
            </div>

            <div class="section">
                <h2>🛡️ Regularization</h2>

                <div class="algorithm-box">
                    <h4>🎯 Controlling Model Complexity</h4>
                    <p>Regularization adds penalty terms to prevent overfitting:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        Regularized Loss = Data Loss + λ × Complexity Penalty
                    </div>
                    <p><strong>λ (lambda):</strong> Regularization parameter controlling the tradeoff</p>
                </div>

                <div class="regularization-grid">
                    <div class="method-card">
                        <h5>📏 L2 Regularization (Ridge)</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Loss = MSE + λ∑wᵢ²
                        </div>
                        <p><strong>Effect:</strong> Shrinks weights toward zero</p>
                        <p><strong>Advantages:</strong></p>
                        <ul>
                            <li>Differentiable everywhere</li>
                            <li>Stable numerical solution</li>
                            <li>Keeps all features</li>
                        </ul>
                        <p><strong>Use when:</strong> All features potentially relevant</p>
                    </div>

                    <div class="method-card">
                        <h5>📐 L1 Regularization (Lasso)</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Loss = MSE + λ∑|wᵢ|
                        </div>
                        <p><strong>Effect:</strong> Forces some weights to exactly zero</p>
                        <p><strong>Advantages:</strong></p>
                        <ul>
                            <li>Automatic feature selection</li>
                            <li>Sparse solutions</li>
                            <li>Interpretable models</li>
                        </ul>
                        <p><strong>Use when:</strong> Feature selection desired</p>
                    </div>

                    <div class="method-card">
                        <h5>🎯 Elastic Net</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            Loss = MSE + λ₁∑|wᵢ| + λ₂∑wᵢ²
                        </div>
                        <p><strong>Effect:</strong> Combines L1 and L2 penalties</p>
                        <p><strong>Advantages:</strong></p>
                        <ul>
                            <li>Feature selection + grouping</li>
                            <li>Handles correlated features</li>
                            <li>More stable than pure Lasso</li>
                        </ul>
                        <p><strong>Use when:</strong> Best of both worlds needed</p>
                    </div>

                    <div class="method-card">
                        <h5>🛑 Early Stopping</h5>
                        <p><strong>Idea:</strong> Stop training before convergence</p>
                        <p><strong>Method:</strong></p>
                        <ul>
                            <li>Monitor validation error</li>
                            <li>Stop when validation error increases</li>
                            <li>Implicit regularization</li>
                        </ul>
                        <p><strong>Advantages:</strong></p>
                        <ul>
                            <li>Computationally efficient</li>
                            <li>Works with any optimization</li>
                            <li>No hyperparameter tuning</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🧠 Neural Networks</h2>

                <div class="neural-layers">
                    <h4>🔗 Building Blocks of Deep Learning</h4>
                    <p>Neural networks are compositions of simple functions that can approximate complex patterns</p>
                    
                    <div class="layer-demo">
                        <strong>Input Layer:</strong> x₁, x₂, ..., xₚ (features)
                    </div>
                    <div class="layer-demo">
                        <strong>Hidden Layer 1:</strong> h₁ⱼ = σ(∑wᵢⱼxᵢ + bⱼ)
                    </div>
                    <div class="layer-demo">
                        <strong>Hidden Layer 2:</strong> h₂ₖ = σ(∑wⱼₖh₁ⱼ + bₖ)
                    </div>
                    <div class="layer-demo">
                        <strong>Output Layer:</strong> ŷ = ∑wₖh₂ₖ + b
                    </div>
                </div>

                <div class="example-box">
                    <h4>⚡ Activation Functions</h4>
                    <p><strong>Role:</strong> Add non-linearity to enable complex patterns</p>
                    
                    <table class="tradeoff-table">
                        <tr>
                            <th>Function</th>
                            <th>Formula</th>
                            <th>Pros</th>
                            <th>Cons</th>
                        </tr>
                        <tr>
                            <td><strong>ReLU</strong></td>
                            <td>max(0, x)</td>
                            <td>Fast, sparse, no vanishing gradient</td>
                            <td>Dead neurons</td>
                        </tr>
                        <tr>
                            <td><strong>Sigmoid</strong></td>
                            <td>1/(1+e^(-x))</td>
                            <td>Smooth, interpretable</td>
                            <td>Vanishing gradient, saturation</td>
                        </tr>
                        <tr>
                            <td><strong>Tanh</strong></td>
                            <td>(e^x - e^(-x))/(e^x + e^(-x))</td>
                            <td>Zero-centered</td>
                            <td>Vanishing gradient</td>
                        </tr>
                    </table>
                </div>

                <div class="theory-box">
                    <h4>🌟 Universal Approximation Theorem</h4>
                    <p>A neural network with:</p>
                    <ul>
                        <li>One hidden layer</li>
                        <li>Enough neurons</li>
                        <li>Non-linear activation</li>
                    </ul>
                    <p><strong>Can approximate any continuous function arbitrarily well!</strong></p>
                    <p><strong>Caveat:</strong> Says nothing about how to find the weights or how many neurons needed</p>
                </div>
            </div>

            <div class="section">
                <h2>🎯 Model Selection and Validation</h2>

                <div class="ml-methods">
                    <div class="method-card">
                        <h5>📊 Cross-Validation</h5>
                        <p><strong>k-fold CV:</strong></p>
                        <ol>
                            <li>Split data into k folds</li>
                            <li>Train on k-1 folds</li>
                            <li>Validate on remaining fold</li>
                            <li>Repeat k times</li>
                            <li>Average performance</li>
                        </ol>
                        <p><strong>Advantage:</strong> Uses all data for both training and validation</p>
                    </div>

                    <div class="method-card">
                        <h5>🔍 Information Criteria</h5>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            AIC = -2log(L) + 2k
                            <br>BIC = -2log(L) + k×log(n)
                        </div>
                        <p><strong>Idea:</strong> Balance fit vs complexity</p>
                        <p><strong>Use:</strong> Lower is better</p>
                        <p><strong>BIC:</strong> Stronger penalty for complexity</p>
                    </div>

                    <div class="method-card">
                        <h5>🎲 Bootstrap</h5>
                        <p><strong>Procedure:</strong></p>
                        <ol>
                            <li>Sample n observations with replacement</li>
                            <li>Fit model to bootstrap sample</li>
                            <li>Evaluate on out-of-bag data</li>
                            <li>Repeat B times</li>
                        </ol>
                        <p><strong>Advantage:</strong> Provides uncertainty quantification</p>
                    </div>
                </div>

                <div class="algorithm-box">
                    <h4>⚠️ Data Splitting Best Practices</h4>
                    <ul>
                        <li><strong>Training set (60%):</strong> Fit model parameters</li>
                        <li><strong>Validation set (20%):</strong> Tune hyperparameters</li>
                        <li><strong>Test set (20%):</strong> Final unbiased evaluation</li>
                    </ul>
                    <p><strong>Golden rule:</strong> Never touch the test set until the very end!</p>
                </div>
            </div>

            <div class="section">
                <h2>📈 Algorithmic Stability</h2>

                <div class="theory-box">
                    <h4>🔒 Why Stability Matters</h4>
                    <p>An algorithm is <span class="highlight">stable</span> if small changes to the training data lead to small changes in the learned model</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        <strong>β-stability:</strong> Algorithm A is β-stable if:
                        <br>|ℓ(A(S), z') - ℓ(A(S^(i)), z')| ≤ β
                    </div>
                    <p>where S^(i) is S with one example changed</p>
                </div>

                <div class="example-box">
                    <h4>🎯 Stability ⟺ Generalization</h4>
                    <p><strong>Key insight:</strong> Stable algorithms generalize well!</p>
                    <div class="formula-box">
                        <strong>Generalization bound:</strong> For β-stable algorithm:
                        <br>E[R(Â)] - R̂(Â) ≤ β
                    </div>
                    <p><strong>Practical implications:</strong></p>
                    <ul>
                        <li>Regularization increases stability</li>
                        <li>Large datasets increase stability</li>
                        <li>Complex models can be unstable</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>🔍 Information-Theoretic Bounds</h2>

                <div class="theory-box">
                    <h4>📊 Mutual Information Bound</h4>
                    <p>The generalization gap is bounded by how much the algorithm "remembers" about the training data:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        E[R(Â) - R̂(Â)] ≤ √(I(A;S)/(2m))
                    </div>
                    <p>where I(A;S) is mutual information between algorithm and training set</p>
                </div>

                <div class="example-box">
                    <h4>🎯 Implications for Deep Learning</h4>
                    <p><strong>Challenge:</strong> Deep networks can memorize random labels</p>
                    <p><strong>Question:</strong> Why do they still generalize on real data?</p>
                    <p><strong>Partial answers:</strong></p>
                    <ul>
                        <li>SGD has implicit regularization</li>
                        <li>Real data has structure that random data lacks</li>
                        <li>Overparameterization can help generalization</li>
                        <li>The bound may not be tight</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>🚀 Modern Developments</h2>

                <div class="ml-methods">
                    <div class="method-card">
                        <h5>🎯 Double Descent</h5>
                        <p><strong>Observation:</strong> Test error can decrease even after overfitting</p>
                        <p><strong>Phases:</strong></p>
                        <ol>
                            <li>Underparameterized: Classical bias-variance</li>
                            <li>Interpolation threshold: Peak test error</li>
                            <li>Overparameterized: Error decreases again</li>
                        </ol>
                        <p><strong>Implication:</strong> More parameters can help!</p>
                    </div>

                    <div class="method-card">
                        <h5>🔄 Meta-Learning</h5>
                        <p><strong>Goal:</strong> Learn to learn quickly</p>
                        <p><strong>Approach:</strong> Train on many tasks to acquire learning algorithm</p>
                        <p><strong>Applications:</strong></p>
                        <ul>
                            <li>Few-shot learning</li>
                            <li>Hyperparameter optimization</li>
                            <li>Neural architecture search</li>
                        </ul>
                    </div>

                    <div class="method-card">
                        <h5>🎲 Adversarial Robustness</h5>
                        <p><strong>Problem:</strong> Small perturbations can fool models</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Adversarial training</li>
                            <li>Certified defenses</li>
                            <li>Robust optimization</li>
                        </ul>
                        <p><strong>Tradeoff:</strong> Robustness vs accuracy</p>
                    </div>
                </div>

                <div class="algorithm-box">
                    <h4>🌟 The Future is Bright</h4>
                    <p><strong>Emerging areas:</strong></p>
                    <ul>
                        <li><strong>Causal ML:</strong> Going beyond correlation</li>
                        <li><strong>Federated learning:</strong> Privacy-preserving ML</li>
                        <li><strong>Continual learning:</strong> Learning without forgetting</li>
                        <li><strong>Interpretable ML:</strong> Understanding model decisions</li>
                        <li><strong>Quantum ML:</strong> Leveraging quantum computing</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>⚠️ Common Pitfalls</h2>

                <div class="ml-methods">
                    <div class="theory-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);">
                        <h4>❌ Data leakage</h4>
                        <p><strong>Problem:</strong> Future information leaks into training</p>
                        <p><strong>Examples:</strong> Using tomorrow's prices, data preprocessing on full dataset</p>
                        <p><strong>Solution:</strong> Strict temporal splits, proper cross-validation</p>
                    </div>

                    <div class="theory-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);">
                        <h4>❌ Overfitting to validation set</h4>
                        <p><strong>Problem:</strong> Hyperparameter tuning on same validation set</p>
                        <p><strong>Result:</strong> Overly optimistic performance estimates</p>
                        <p><strong>Solution:</strong> Nested cross-validation, hold-out test set</p>
                    </div>

                    <div class="theory-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);">
                        <h4>❌ Ignoring class imbalance</h4>
                        <p><strong>Problem:</strong> Rare classes get ignored</p>
                        <p><strong>Solutions:</strong> Stratified sampling, cost-sensitive learning, SMOTE</p>
                        <p><strong>Metrics:</strong> Use F1, AUC, not just accuracy</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="#" class="nav-btn">← Chapter 21: High-Dimensional Statistics</a>
            <a href="#" class="nav-btn">Chapter 23: Classification →</a>
        </div>
    </div>
</body>
</html>