            <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 16: Logistic Regression</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 50%, #45b7d1 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: 'p=e^x/(1+e^x)';
            position: absolute;
            top: 50%;
            left: -20%;
            transform: translateY(-50%);
            font-size: 4em;
            opacity: 0.1;
            animation: scroll 15s linear infinite;
        }

        @keyframes scroll {
            0% { left: -20%; }
            100% { left: 120%; }
        }

        .header h1 {
            position: relative;
            z-index: 1;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header p {
            position: relative;
            z-index: 1;
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 40px;
            padding: 30px;
            border-radius: 15px;
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            border-left: 5px solid #45b7d1;
            transition: all 
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            border-left: 5px solid #45b7d1;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .section::before {
            content: '';
            position: absolute;
            top: -100%;
            right: -100%;
            width: 200px;
            height: 200px;
            background: radial-gradient(circle, rgba(69,183,209,0.1) 0%, transparent 70%);
            border-radius: 50%;
            animation: pulse 4s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); opacity: 0.3; }
            50% { transform: scale(1.2); opacity: 0.1; }
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(69, 183, 209, 0.2);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #45b7d1;
            padding-bottom: 10px;
            display: inline-block;
            position: relative;
            z-index: 1;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
            background: linear-gradient(135deg, #45b7d1, #2196f3);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            position: relative;
            z-index: 1;
        }

        .logistic-concept {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 25px rgba(69, 183, 209, 0.3);
            transition: all 0.3s ease;
            position: relative;
        }

        .logistic-concept::before {
            content: '📊';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 3em;
            opacity: 0.3;
        }

        .logistic-concept:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 30px rgba(69, 183, 209, 0.4);
        }

        .logistic-concept h4 {
            font-size: 1.4em;
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }

        .formula-box {
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #ff9800;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            box-shadow: 0 8px 20px rgba(255, 152, 0, 0.2);
            position: relative;
            overflow: hidden;
        }

        .formula-box::before {
            content: 'exp';
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 1.5em;
            opacity: 0.3;
            color: #e65100;
            font-style: italic;
        }

        .example-box {
            background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #9c27b0;
            box-shadow: 0 8px 20px rgba(156, 39, 176, 0.2);
            position: relative;
        }

        .example-box h4 {
            color: #6a1b9a;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .theorem-box {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(69, 183, 209, 0.3);
            position: relative;
        }

        .theorem-box::before {
            content: '∀';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 1.5em;
            opacity: 0.7;
            font-style: italic;
        }

        .theorem-box h4 {
            font-size: 1.3em;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        .problem-box {
            background: linear-gradient(135deg, #ffcdd2 0%, #f8bbd9 100%);
            color: #c62828;
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #e91e63;
            box-shadow: 0 8px 20px rgba(233, 30, 99, 0.2);
        }

        .problem-box h4 {
            font-size: 1.2em;
            margin-bottom: 10px;
        }

        .sigmoid-visual {
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            border: 3px solid #ff9800;
            text-align: center;
        }

        .curve-demo {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 15px 0;
            font-size: 1.1em;
            font-weight: bold;
            flex-wrap: wrap;
        }

        .curve-demo .arrow {
            color: #e65100;
            font-size: 2em;
        }

        .curve-point {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 10px 15px;
            border-radius: 15px;
            margin: 5px;
            min-width: 80px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(69, 183, 209, 0.3);
            transition: transform 0.3s ease;
        }

        .curve-point:hover {
            transform: scale(1.05);
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin: 20px 0;
        }

        .comparison-table th {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        .comparison-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }

        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .diagnostic-box {
            background: linear-gradient(135deg, #00e676 0%, #00c853 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(0, 230, 118, 0.3);
        }

        .diagnostic-box h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
            text-align: center;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 15px;
        }

        .mle-visual {
            background: white;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            text-align: center;
            box-shadow: 0 15px 35px rgba(0,0,0,0.1);
        }

        .iteration-steps {
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }

        .iteration-step {
            background: linear-gradient(135deg, #45b7d1, #2196f3);
            color: white;
            padding: 20px;
            border-radius: 15px;
            min-width: 150px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
            transition: transform 0.3s ease;
        }

        .iteration-step:hover {
            transform: translateY(-5px);
        }

        ul, ol {
            margin-left: 25px;
            margin-top: 10px;
        }

        li {
            margin-bottom: 8px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        }

        .nav-btn {
            padding: 12px 25px;
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 5px 15px rgba(69, 183, 209, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(69, 183, 209, 0.4);
        }

        @media (max-width: 768px) {
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .content {
                padding: 20px;
            }

            .curve-demo {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chapter 16: Logistic Regression</h1>
            <p>Modeling Binary Outcomes and Categorical Data</p>
        </div>

        <div class="content">
            <div class="section">
                <h2>🎯 Why Logistic Regression?</h2>
                <p>When your outcome is <strong>binary</strong> (yes/no, success/failure, diseased/healthy), linear regression fails! Logistic regression is the perfect tool for modeling probabilities and making classification decisions.</p>
                
                <div class="example-box">
                    <h4>🏥 Medical Diagnosis Example</h4>
                    <p><strong>Question:</strong> Does a patient have disease based on symptoms?</p>
                    <p><strong>Problem with linear regression:</strong> Predictions can be negative or > 1</p>
                    <p><strong>Logistic solution:</strong> Model probability of disease ∈ [0,1]</p>
                </div>

                <div class="problem-box">
                    <h4>❌ Problems with Linear Regression for Binary Outcomes</h4>
                    <ul>
                        <li><strong>Predicted values outside [0,1]:</strong> Doesn't make sense for probabilities</li>
                        <li><strong>Non-normal errors:</strong> Binary data isn't normally distributed</li>
                        <li><strong>Heteroscedasticity:</strong> Variance depends on mean</li>
                    </ul>
                </div>

                <div class="sigmoid-visual">
                    <h4>📈 The S-Curve Solution</h4>
                    <div class="curve-demo">
                        <span class="curve-point">0</span>
                        <span class="arrow">→</span>
                        <span class="curve-point">S-curve</span>
                        <span class="arrow">→</span>
                        <span class="curve-point">1</span>
                    </div>
                    <p>The <strong>logistic function</strong> maps any real number to (0,1)</p>
                </div>
            </div>

            <div class="section">
                <h2>📊 The Logistic Function</h2>

                <div class="logistic-concept">
                    <h4>🌊 The Sigmoid Curve</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        p(x) = e^(β₀ + β₁x) / (1 + e^(β₀ + β₁x)) = 1 / (1 + e^(-β₀ - β₁x))
                    </div>
                    <p><strong>Properties:</strong></p>
                    <ul>
                        <li>S-shaped curve</li>
                        <li>Always between 0 and 1</li>
                        <li>Smooth and differentiable</li>
                        <li>lim_{x→-∞} p(x) = 0, lim_{x→∞} p(x) = 1</li>
                    </ul>
                </div>

                <div class="theorem-box">
                    <h4>🔗 The Logit Transformation</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        logit(p) = ln(p/(1-p)) = β₀ + β₁x₁ + ... + βₚxₚ
                    </div>
                    <p><strong>Key insight:</strong> The log-odds is linear in the predictors!</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>📊 Odds and Odds Ratios</h4>
                        <p><strong>Odds:</strong> odds = p/(1-p)</p>
                        <p><strong>If p = 0.8:</strong> odds = 0.8/0.2 = 4</p>
                        <p><strong>Interpretation:</strong> "4 to 1 odds" or "4 times as likely"</p>
                    </div>

                    <div class="example-box">
                        <h4>🎯 Odds Ratio Interpretation</h4>
                        <p><strong>For one-unit increase in x:</strong></p>
                        <p><strong>OR = e^β:</strong> Multiplicative change in odds</p>
                        <p><strong>If β = 0.693:</strong> OR = 2 (odds double)</p>
                        <p><strong>If β = -0.693:</strong> OR = 0.5 (odds halve)</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🔬 Maximum Likelihood Estimation</h2>

                <div class="theorem-box">
                    <h4>📊 The Likelihood Function</h4>
                    <p>For binary data Y₁, Y₂, ..., Yₙ:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        L(β) = ∏ᵢ pᵢ^yᵢ (1-pᵢ)^(1-yᵢ)
                    </div>
                    <p>where pᵢ = P(Yᵢ = 1|xᵢ) = logistic function</p>
                </div>

                <div class="logistic-concept">
                    <h4>📈 Log-Likelihood</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        ℓ(β) = Σᵢ [yᵢ log(pᵢ) + (1-yᵢ) log(1-pᵢ)]
                        <br>= Σᵢ [yᵢxᵢ'β - log(1 + e^(xᵢ'β))]
                    </div>
                </div>

                <div class="mle-visual">
                    <h2>🔄 Newton-Raphson Algorithm</h2>
                    <p>No closed-form solution - need iterative methods!</p>
                </div>

                <div class="iteration-steps">
                    <div class="iteration-step">
                        <h4>📊 Score Function</h4>
                        <div class="formula-box">
                            S(β) = X'(y - p)
                        </div>
                    </div>

                    <div class="iteration-step">
                        <h4>🔢 Hessian</h4>
                        <div class="formula-box">
                            H(β) = -X'WX
                        </div>
                        <small>W = diag(pᵢ(1-pᵢ))</small>
                    </div>

                    <div class="iteration-step">
                        <h4>🔄 Update Rule</h4>
                        <div class="formula-box">
                            β^(t+1) = β^(t) - H⁻¹S
                        </div>
                    </div>
                </div>

                <div class="example-box">
                    <h4>💻 IRLS (Iteratively Reweighted Least Squares)</h4>
                    <p><strong>Equivalent form:</strong> Each iteration is a weighted least squares problem</p>
                    <div class="formula-box">
                        β^(t+1) = (X'W^(t)X)⁻¹X'W^(t)z^(t)
                    </div>
                    <p>where z^(t) = Xβ^(t) + W⁻¹(y - p^(t)) is the "working response"</p>
                </div>
            </div>

            <div class="section">
                <h2>📊 Inference and Hypothesis Testing</h2>

                <div class="theorem-box">
                    <h4>🎯 Asymptotic Distribution</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        √n(β̂ - β) ⇒ N(0, I⁻¹(β))
                    </div>
                    <p>where I(β) is the Fisher information matrix</p>
                </div>

                <div class="grid-3">
                    <div class="logistic-concept">
                        <h4>📊 Wald Tests</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            z = β̂ⱼ/se(β̂ⱼ) ~ N(0,1)
                        </div>
                        <p><strong>For individual parameters</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>🎯 Likelihood Ratio Tests</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            LRT = 2[ℓ(β̂) - ℓ(β̂₀)] ~ χ²_q
                        </div>
                        <p><strong>For nested models</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>📈 Score Tests</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            LM = S(β̂₀)'I⁻¹(β̂₀)S(β̂₀) ~ χ²_q
                        </div>
                        <p><strong>Only need null model</strong></p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>🔍 Confidence Intervals</h4>
                    <p><strong>For coefficients:</strong> β̂ⱼ ± z_{α/2} × se(β̂ⱼ)</p>
                    <p><strong>For odds ratios:</strong> exp(β̂ⱼ ± z_{α/2} × se(β̂ⱼ))</p>
                    <p><strong>Profile likelihood intervals:</strong> Often more accurate for small samples</p>
                </div>
            </div>

            <div class="section">
                <h2>🩺 Model Diagnostics</h2>

                <div class="diagnostic-box">
                    <h4>🔍 Goodness-of-Fit Assessment</h4>
                    <p>How well does our model fit the data?</p>
                </div>

                <div class="grid-2">
                    <div class="logistic-concept">
                        <h4>📊 Deviance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            D = 2[ℓ_saturated - ℓ(β̂)]
                        </div>
                        <p><strong>Measures:</strong> Distance from perfect fit</p>
                    </div>

                    <div class="logistic-concept">
                        <h4>🎯 Pearson Chi-square</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            X² = Σᵢ (yᵢ - p̂ᵢ)² / (p̂ᵢ(1-p̂ᵢ))
                        </div>
                        <p><strong>Alternative:</strong> Goodness-of-fit measure</p>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>🏥 Hosmer-Lemeshow Test</h4>
                    <p><strong>Procedure:</strong></p>
                    <ol>
                        <li>Group observations by predicted probabilities</li>
                        <li>Compare observed vs expected in each group</li>
                        <li>Test statistic follows χ² distribution</li>
                    </ol>
                    <p><strong>H₀:</strong> Model fits well (want large p-value!)</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>📊 Residuals for Binary Data</h4>
                        <p><strong>Pearson residuals:</strong></p>
                        <div class="formula-box">
                            rᵢᴾ = (yᵢ - p̂ᵢ)/√(p̂ᵢ(1-p̂ᵢ))
                        </div>
                        <p><strong>Deviance residuals:</strong></p>
                        <div class="formula-box">
                            rᵢᴰ = sign(yᵢ - p̂ᵢ)√[2yᵢlog(yᵢ/p̂ᵢ) + 2(1-yᵢ)log((1-yᵢ)/(1-p̂ᵢ))]
                        </div>
                    </div>

                    <div class="example-box">
                        <h4>🎯 Influential Observations</h4>
                        <p><strong>Cook's distance:</strong></p>
                        <div class="formula-box">
                            Dᵢ = (rᵢᴾ)²/(p+1) × hᵢᵢ/(1-hᵢᵢ)
                        </div>
                        <p><strong>DFBETAS:</strong> Change in β̂ when removing observation i</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>📈 ROC Curves and Classification</h2>

                <div class="theorem-box">
                    <h4>📊 Receiver Operating Characteristic (ROC)</h4>
                    <p><strong>ROC curve:</strong> Plot of Sensitivity vs (1 - Specificity)</p>
                    <ul>
                        <li><strong>Sensitivity:</strong> P(predict positive | actually positive) = TP/(TP+FN)</li>
                        <li><strong>Specificity:</strong> P(predict negative | actually negative) = TN/(TN+FP)</li>
                    </ul>
                </div>

                <div class="sigmoid-visual">
                    <h4>🎯 Classification Metrics</h4>
                    <table class="comparison-table" style="margin: 20px auto; max-width: 500px;">
                        <tr>
                            <th></th>
                            <th>Predicted +</th>
                            <th>Predicted -</th>
                        </tr>
                        <tr>
                            <td><strong>Actual +</strong></td>
                            <td>TP (True Positive)</td>
                            <td>FN (False Negative)</td>
                        </tr>
                        <tr>
                            <td><strong>Actual -</strong></td>
                            <td>FP (False Positive)</td>
                            <td>TN (True Negative)</td>
                        </tr>
                    </table>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>📏 Area Under Curve (AUC)</h4>
                        <ul>
                            <li><strong>AUC = 0.5:</strong> No discriminatory ability (random)</li>
                            <li><strong>AUC = 0.7:</strong> Acceptable discrimination</li>
                            <li><strong>AUC = 0.8:</strong> Excellent discrimination</li>
                            <li><strong>AUC = 1.0:</strong> Perfect discrimination</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>⚖️ Choosing Classification Threshold</h4>
                        <p><strong>Default:</strong> p̂ = 0.5</p>
                        <p><strong>Medical testing:</strong> Lower threshold (don't miss disease)</p>
                        <p><strong>Spam detection:</strong> Higher threshold (don't block real email)</p>
                        <p><strong>Optimal:</strong> Maximize (Sensitivity + Specificity)</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🏗️ Extensions and Variations</h2>

                <div class="theorem-box">
                    <h4>🎯 Multinomial Logistic Regression</h4>
                    <p>For outcomes with K > 2 categories:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        P(Y = j|x) = exp(x'βⱼ) / Σₖ₌₁ᴷ exp(x'βₖ)
                    </div>
                    <p><strong>Constraint:</strong> Set β₁ = 0 (reference category)</p>
                </div>

                <div class="grid-3">
                    <div class="logistic-concept">
                        <h4>📊 Ordinal Logistic</h4>
                        <p><strong>For ordered categories</strong></p>
                        <p>Examples: Likert scales, severity ratings</p>
                        <p><strong>Uses cumulative probabilities</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>🎯 Mixed Effects</h4>
                        <p><strong>For clustered/repeated data</strong></p>
                        <p>Examples: Students within schools</p>
                        <p><strong>Accounts for correlation</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>📈 Regularized Logistic</h4>
                        <p><strong>Ridge:</strong> L2 penalty</p>
                        <p><strong>LASSO:</strong> L1 penalty</p>
                        <p><strong>Elastic Net:</strong> L1 + L2</p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>🌟 Generalized Linear Models (GLMs)</h4>
                    <p>Logistic regression is a special case of GLMs!</p>
                    <p><strong>Components:</strong></p>
                    <ul>
                        <li><strong>Random component:</strong> Y ~ Binomial</li>
                        <li><strong>Systematic component:</strong> η = Xβ</li>
                        <li><strong>Link function:</strong> logit(μ) = η</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>⚠️ Common Pitfalls and Solutions</h2>

                <div class="grid-2">
                    <div class="problem-box">
                        <h4>❌ Complete Separation</h4>
                        <p><strong>Problem:</strong> Perfect classification by one variable</p>
                        <p><strong>Result:</strong> Infinite coefficient estimates</p>
                        <p><strong>Solution:</strong> Penalized likelihood, Firth regression</p>
                    </div>

                    <div class="problem-box">
                        <h4>❌ Quasi-Complete Separation</h4>
                        <p><strong>Problem:</strong> Nearly perfect separation</p>
                        <p><strong>Result:</strong> Very large standard errors</p>
                        <p><strong>Detection:</strong> Extremely large coefficients</p>
                    </div>
                </div>

                <div class="problem-box">
                    <h4>❌ Misinterpreting Coefficients</h4>
                    <p><strong>Problem:</strong> Thinking β represents change in probability</p>
                    <p><strong>Reality:</strong> β represents change in log-odds</p>
                    <p><strong>Solution:</strong> Report odds ratios: OR = e^β</p>
                </div>

                <div class="grid-2">
                    <div class="problem-box">
                        <h4>❌ Ignoring Non-linearity</h4>
                        <p><strong>Problem:</strong> Assuming linear relationship in logit</p>
                        <p><strong>Solution:</strong> Add polynomial terms, splines, or interactions</p>
                    </div>

                    <div class="problem-box">
                        <h4>❌ Small Sample Issues</h4>
                        <p><strong>Problem:</strong> Asymptotic theory breaks down</p>
                        <p><strong>Rule of thumb:</strong> Need 10+ events per predictor</p>
                        <p><strong>Solution:</strong> Exact methods, penalized likelihood</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🎯 Model Selection and Regularization</h2>

                <div class="theorem-box">
                    <h4>📊 Information Criteria</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        AIC = -2ℓ(β̂) + 2p
                        <br>BIC = -2ℓ(β̂) + p log(n)
                    </div>
                    <p><strong>Lower is better!</strong> Balance fit and complexity</p>
                </div>

                <div class="mle-visual">
                    <h2>🔧 Regularized Logistic Regression</h2>
                    <p>Add penalties to prevent overfitting in high dimensions!</p>
                </div>

                <div class="iteration-steps">
                    <div class="iteration-step">
                        <h4>🔵 Ridge Logistic</h4>
                        <div class="formula-box">
                            β̂ᴿⁱᵈᵍᵉ = argmin{-ℓ(β) + λΣβⱼ²}
                        </div>
                        <p><strong>Shrinks coefficients</strong></p>
                    </div>

                    <div class="iteration-step">
                        <h4>🔶 LASSO Logistic</h4>
                        <div class="formula-box">
                            β̂ᴸᴬˢˢᴼ = argmin{-ℓ(β) + λΣ|βⱼ|}
                        </div>
                        <p><strong>Variable selection</strong></p>
                    </div>

                    <div class="iteration-step">
                        <h4>🟣 Elastic Net</h4>
                        <div class="formula-box">
                            β̂ᴱᴺ = argmin{-ℓ(β) + λ₁Σ|βⱼ| + λ₂Σβⱼ²}
                        </div>
                        <p><strong>Best of both</strong></p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>🔍 Cross-Validation for Logistic Regression</h4>
                    <p><strong>k-fold CV:</strong> Split data, train on k-1 folds, test on 1</p>
                    <p><strong>Metrics to optimize:</strong></p>
                    <ul>
                        <li><strong>Log-likelihood:</strong> Probabilistic scoring</li>
                        <li><strong>Classification accuracy:</strong> Percent correct</li>
                        <li><strong>AUC:</strong> Area under ROC curve</li>
                        <li><strong>F1-score:</strong> Harmonic mean of precision and recall</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>🔗 Connections to Other Chapters</h2>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>📊 To Linear Regression (Chapter 14)</h4>
                        <ul>
                            <li><strong>Extension:</strong> From continuous to binary outcomes</li>
                            <li><strong>Similar concepts:</strong> Coefficients, inference, diagnostics</li>
                            <li><strong>Key difference:</strong> Link function and distribution</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>📈 To Convergence (Chapter 6)</h4>
                        <ul>
                            <li><strong>Asymptotic normality:</strong> √n(β̂ - β) ⇒ N(0,V)</li>
                            <li><strong>Consistency:</strong> β̂ₙ →ᵖ β</li>
                            <li><strong>Central limit theorem:</strong> Foundation for inference</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>🔬 To Hypothesis Testing (Chapter 11)</h4>
                        <ul>
                            <li><strong>Wald tests:</strong> Individual coefficient tests</li>
                            <li><strong>Likelihood ratio tests:</strong> Nested model comparison</li>
                            <li><strong>Score tests:</strong> Efficient testing procedures</li>
                        </ul>
                    </div>
                </div>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>📊 To Nonparametric Methods (Chapter 17)</h4>
                        <ul>
                            <li><strong>GAMs:</strong> Smooth functions in logistic models</li>
                            <li><strong>Classification trees:</strong> Alternative to logistic regression</li>
                            <li><strong>Kernel methods:</strong> Nonparametric classification</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>🤖 To Machine Learning (Chapter 23)</h4>
                        <ul>
                            <li><strong>Classification:</strong> Logistic regression as ML method</li>
                            <li><strong>Regularization:</strong> Ridge, LASSO, Elastic Net</li>
                            <li><strong>Cross-validation:</strong> Model selection and evaluation</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>📊 To High-Dimensional (Chapter 21)</h4>
                        <ul>
                            <li><strong>Sparse models:</strong> LASSO for variable selection</li>
                            <li><strong>Regularization theory:</strong> Oracle properties</li>
                            <li><strong>Computational methods:</strong> Coordinate descent</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>💼 Real-World Applications</h2>

                <div class="grid-2">
                    <div class="diagnostic-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);">
                        <h4>🏥 Medical Applications</h4>
                        <ul>
                            <li><strong>Disease diagnosis:</strong> Predict disease from symptoms</li>
                            <li><strong>Treatment response:</strong> Will patient respond to therapy?</li>
                            <li><strong>Risk scoring:</strong> Probability of adverse events</li>
                            <li><strong>Clinical trials:</strong> Binary endpoints</li>
                        </ul>
                    </div>

                    <div class="diagnostic-box" style="background: linear-gradient(135deg, #4ecdc4 0%, #44a08d 100%);">
                        <h4>💰 Business Applications</h4>
                        <ul>
                            <li><strong>Marketing:</strong> Will customer buy product?</li>
                            <li><strong>Credit scoring:</strong> Will borrower default?</li>
                            <li><strong>Churn prediction:</strong> Will customer leave?</li>
                            <li><strong>A/B testing:</strong> Did intervention work?</li>
                        </ul>
                    </div>
                </div>

                <div class="example-box">
                    <h4>🎯 Example: Email Spam Detection</h4>
                    <p><strong>Outcome:</strong> Y = 1 if spam, 0 if legitimate</p>
                    <p><strong>Predictors:</strong> Word frequencies, sender info, etc.</p>
                    <p><strong>Model:</strong> logit(P(spam)) = β₀ + β₁(frequency of "free") + β₂(ALL CAPS) + ...</p>
                    <p><strong>Interpretation:</strong> OR > 1 means higher spam probability</p>
                    <p><strong>Decision rule:</strong> Classify as spam if p̂ > threshold</p>
                </div>
            </div>

            <div class="section">
                <h2>🌟 Summary and Key Takeaways</h2>

                <div class="diagnostic-box">
                    <h4>🎯 Essential Concepts</h4>
                    <ul>
                        <li><strong>Logistic function:</strong> Maps real line to (0,1) for probabilities</li>
                        <li><strong>Logit transformation:</strong> Makes relationship linear in parameters</li>
                        <li><strong>Maximum likelihood:</strong> No closed form, need iterative methods</li>
                        <li><strong>Odds ratios:</strong> Natural interpretation of coefficients</li>
                        <li><strong>ROC curves:</strong> Evaluate classification performance</li>
                        <li><strong>Regularization:</strong> Prevents overfitting in high dimensions</li>
                    </ul>
                </div>

                <div class="sigmoid-visual">
                    <h4>🚀 The Big Picture</h4>
                    <p>Logistic regression extends linear modeling to <strong>binary outcomes</strong>. Key advantages:</p>
                    <ul style="text-align: left; max-width: 600px; margin: 0 auto;">
                        <li>Outputs interpretable probabilities</li>
                        <li>No distributional assumptions on predictors</li>
                        <li>Robust and widely applicable</li>
                        <li>Foundation for many machine learning methods</li>
                        <li>Extends naturally to multiple categories</li>
                    </ul>
                </div>

                <div class="theorem-box">
                    <h4>🎓 Best Practices</h4>
                    <ul>
                        <li><strong>Check for separation:</strong> Inspect coefficient magnitudes</li>
                        <li><strong>Assess fit:</strong> Use Hosmer-Lemeshow test, ROC curves</li>
                        <li><strong>Validate predictions:</strong> Cross-validation essential</li>
                        <li><strong>Report odds ratios:</strong> More interpretable than coefficients</li>
                        <li><strong>Consider regularization:</strong> Especially with many predictors</li>
                        <li><strong>Choose threshold wisely:</strong> Consider costs of false positives/negatives</li>
                    </ul>
                </div>

                <div class="example-box">
                    <h4>🔮 Advanced Topics to Explore</h4>
                    <ul>
                        <li><strong>Bayesian logistic regression:</strong> Incorporates prior information</li>
                        <li><strong>Exact logistic regression:</strong> For small samples</li>
                        <li><strong>Robust logistic regression:</strong> Handles outliers</li>
                        <li><strong>Nonlinear logistic models:</strong> GAMs, neural networks</li>
                        <li><strong>Multilevel models:</strong> Hierarchical/clustered data</li>
                    </ul>
                </div>

                <div class="mle-visual">
                    <h2>💡 Why Logistic Regression Matters</h2>
                    <p>In our data-driven world, many decisions are binary: approve/deny, buy/don't buy, sick/healthy. Logistic regression provides a <strong>principled statistical framework</strong> for making these decisions based on data, with proper uncertainty quantification and interpretable results.</p>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="#" class="nav-btn">← Chapter 15: Multiple Regression</a>
            <a href="#" class="nav-btn">Chapter 17: Nonparametric Smoothing →</a>
        </div>
    </div>
</body>
</html>