            <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 16: Logistic Regression</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 50%, #45b7d1 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: 'p=e^x/(1+e^x)';
            position: absolute;
            top: 50%;
            left: -20%;
            transform: translateY(-50%);
            font-size: 4em;
            opacity: 0.1;
            animation: scroll 15s linear infinite;
        }

        @keyframes scroll {
            0% { left: -20%; }
            100% { left: 120%; }
        }

        .header h1 {
            position: relative;
            z-index: 1;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header p {
            position: relative;
            z-index: 1;
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 40px;
            padding: 30px;
            border-radius: 15px;
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            border-left: 5px solid #45b7d1;
            transition: all 
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
            border-left: 5px solid #45b7d1;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .section::before {
            content: '';
            position: absolute;
            top: -100%;
            right: -100%;
            width: 200px;
            height: 200px;
            background: radial-gradient(circle, rgba(69,183,209,0.1) 0%, transparent 70%);
            border-radius: 50%;
            animation: pulse 4s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); opacity: 0.3; }
            50% { transform: scale(1.2); opacity: 0.1; }
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(69, 183, 209, 0.2);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #45b7d1;
            padding-bottom: 10px;
            display: inline-block;
            position: relative;
            z-index: 1;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
            background: linear-gradient(135deg, #45b7d1, #2196f3);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            position: relative;
            z-index: 1;
        }

        .logistic-concept {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 25px rgba(69, 183, 209, 0.3);
            transition: all 0.3s ease;
            position: relative;
        }

        .logistic-concept::before {
            content: 'üìä';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 3em;
            opacity: 0.3;
        }

        .logistic-concept:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 30px rgba(69, 183, 209, 0.4);
        }

        .logistic-concept h4 {
            font-size: 1.4em;
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }

        .formula-box {
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #ff9800;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            box-shadow: 0 8px 20px rgba(255, 152, 0, 0.2);
            position: relative;
            overflow: hidden;
        }

        .formula-box::before {
            content: 'exp';
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 1.5em;
            opacity: 0.3;
            color: #e65100;
            font-style: italic;
        }

        .example-box {
            background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #9c27b0;
            box-shadow: 0 8px 20px rgba(156, 39, 176, 0.2);
            position: relative;
        }

        .example-box h4 {
            color: #6a1b9a;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .theorem-box {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(69, 183, 209, 0.3);
            position: relative;
        }

        .theorem-box::before {
            content: '‚àÄ';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 1.5em;
            opacity: 0.7;
            font-style: italic;
        }

        .theorem-box h4 {
            font-size: 1.3em;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        .problem-box {
            background: linear-gradient(135deg, #ffcdd2 0%, #f8bbd9 100%);
            color: #c62828;
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #e91e63;
            box-shadow: 0 8px 20px rgba(233, 30, 99, 0.2);
        }

        .problem-box h4 {
            font-size: 1.2em;
            margin-bottom: 10px;
        }

        .sigmoid-visual {
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            border: 3px solid #ff9800;
            text-align: center;
        }

        .curve-demo {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 15px 0;
            font-size: 1.1em;
            font-weight: bold;
            flex-wrap: wrap;
        }

        .curve-demo .arrow {
            color: #e65100;
            font-size: 2em;
        }

        .curve-point {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 10px 15px;
            border-radius: 15px;
            margin: 5px;
            min-width: 80px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(69, 183, 209, 0.3);
            transition: transform 0.3s ease;
        }

        .curve-point:hover {
            transform: scale(1.05);
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin: 20px 0;
        }

        .comparison-table th {
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        .comparison-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }

        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .diagnostic-box {
            background: linear-gradient(135deg, #00e676 0%, #00c853 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(0, 230, 118, 0.3);
        }

        .diagnostic-box h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
            text-align: center;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 15px;
        }

        .mle-visual {
            background: white;
            border-radius: 20px;
            padding: 30px;
            margin: 20px 0;
            text-align: center;
            box-shadow: 0 15px 35px rgba(0,0,0,0.1);
        }

        .iteration-steps {
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0;
        }

        .iteration-step {
            background: linear-gradient(135deg, #45b7d1, #2196f3);
            color: white;
            padding: 20px;
            border-radius: 15px;
            min-width: 150px;
            text-align: center;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
            transition: transform 0.3s ease;
        }

        .iteration-step:hover {
            transform: translateY(-5px);
        }

        ul, ol {
            margin-left: 25px;
            margin-top: 10px;
        }

        li {
            margin-bottom: 8px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        }

        .nav-btn {
            padding: 12px 25px;
            background: linear-gradient(135deg, #45b7d1 0%, #2196f3 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 5px 15px rgba(69, 183, 209, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(69, 183, 209, 0.4);
        }

        @media (max-width: 768px) {
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .content {
                padding: 20px;
            }

            .curve-demo {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chapter 16: Logistic Regression</h1>
            <p>Modeling Binary Outcomes and Categorical Data</p>
        </div>

        <div class="content">
            <div class="section">
                <h2>üéØ Why Logistic Regression?</h2>
                <p>When your outcome is <strong>binary</strong> (yes/no, success/failure, diseased/healthy), linear regression fails! Logistic regression is the perfect tool for modeling probabilities and making classification decisions.</p>
                
                <div class="example-box">
                    <h4>üè• Medical Diagnosis Example</h4>
                    <p><strong>Question:</strong> Does a patient have disease based on symptoms?</p>
                    <p><strong>Problem with linear regression:</strong> Predictions can be negative or > 1</p>
                    <p><strong>Logistic solution:</strong> Model probability of disease ‚àà [0,1]</p>
                </div>

                <div class="problem-box">
                    <h4>‚ùå Problems with Linear Regression for Binary Outcomes</h4>
                    <ul>
                        <li><strong>Predicted values outside [0,1]:</strong> Doesn't make sense for probabilities</li>
                        <li><strong>Non-normal errors:</strong> Binary data isn't normally distributed</li>
                        <li><strong>Heteroscedasticity:</strong> Variance depends on mean</li>
                    </ul>
                </div>

                <div class="sigmoid-visual">
                    <h4>üìà The S-Curve Solution</h4>
                    <div class="curve-demo">
                        <span class="curve-point">0</span>
                        <span class="arrow">‚Üí</span>
                        <span class="curve-point">S-curve</span>
                        <span class="arrow">‚Üí</span>
                        <span class="curve-point">1</span>
                    </div>
                    <p>The <strong>logistic function</strong> maps any real number to (0,1)</p>
                </div>
            </div>

            <div class="section">
                <h2>üìä The Logistic Function</h2>

                <div class="logistic-concept">
                    <h4>üåä The Sigmoid Curve</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        p(x) = e^(Œ≤‚ÇÄ + Œ≤‚ÇÅx) / (1 + e^(Œ≤‚ÇÄ + Œ≤‚ÇÅx)) = 1 / (1 + e^(-Œ≤‚ÇÄ - Œ≤‚ÇÅx))
                    </div>
                    <p><strong>Properties:</strong></p>
                    <ul>
                        <li>S-shaped curve</li>
                        <li>Always between 0 and 1</li>
                        <li>Smooth and differentiable</li>
                        <li>lim_{x‚Üí-‚àû} p(x) = 0, lim_{x‚Üí‚àû} p(x) = 1</li>
                    </ul>
                </div>

                <div class="theorem-box">
                    <h4>üîó The Logit Transformation</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        logit(p) = ln(p/(1-p)) = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çöx‚Çö
                    </div>
                    <p><strong>Key insight:</strong> The log-odds is linear in the predictors!</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>üìä Odds and Odds Ratios</h4>
                        <p><strong>Odds:</strong> odds = p/(1-p)</p>
                        <p><strong>If p = 0.8:</strong> odds = 0.8/0.2 = 4</p>
                        <p><strong>Interpretation:</strong> "4 to 1 odds" or "4 times as likely"</p>
                    </div>

                    <div class="example-box">
                        <h4>üéØ Odds Ratio Interpretation</h4>
                        <p><strong>For one-unit increase in x:</strong></p>
                        <p><strong>OR = e^Œ≤:</strong> Multiplicative change in odds</p>
                        <p><strong>If Œ≤ = 0.693:</strong> OR = 2 (odds double)</p>
                        <p><strong>If Œ≤ = -0.693:</strong> OR = 0.5 (odds halve)</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üî¨ Maximum Likelihood Estimation</h2>

                <div class="theorem-box">
                    <h4>üìä The Likelihood Function</h4>
                    <p>For binary data Y‚ÇÅ, Y‚ÇÇ, ..., Y‚Çô:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        L(Œ≤) = ‚àè·µ¢ p·µ¢^y·µ¢ (1-p·µ¢)^(1-y·µ¢)
                    </div>
                    <p>where p·µ¢ = P(Y·µ¢ = 1|x·µ¢) = logistic function</p>
                </div>

                <div class="logistic-concept">
                    <h4>üìà Log-Likelihood</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        ‚Ñì(Œ≤) = Œ£·µ¢ [y·µ¢ log(p·µ¢) + (1-y·µ¢) log(1-p·µ¢)]
                        <br>= Œ£·µ¢ [y·µ¢x·µ¢'Œ≤ - log(1 + e^(x·µ¢'Œ≤))]
                    </div>
                </div>

                <div class="mle-visual">
                    <h2>üîÑ Newton-Raphson Algorithm</h2>
                    <p>No closed-form solution - need iterative methods!</p>
                </div>

                <div class="iteration-steps">
                    <div class="iteration-step">
                        <h4>üìä Score Function</h4>
                        <div class="formula-box">
                            S(Œ≤) = X'(y - p)
                        </div>
                    </div>

                    <div class="iteration-step">
                        <h4>üî¢ Hessian</h4>
                        <div class="formula-box">
                            H(Œ≤) = -X'WX
                        </div>
                        <small>W = diag(p·µ¢(1-p·µ¢))</small>
                    </div>

                    <div class="iteration-step">
                        <h4>üîÑ Update Rule</h4>
                        <div class="formula-box">
                            Œ≤^(t+1) = Œ≤^(t) - H‚Åª¬πS
                        </div>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üíª IRLS (Iteratively Reweighted Least Squares)</h4>
                    <p><strong>Equivalent form:</strong> Each iteration is a weighted least squares problem</p>
                    <div class="formula-box">
                        Œ≤^(t+1) = (X'W^(t)X)‚Åª¬πX'W^(t)z^(t)
                    </div>
                    <p>where z^(t) = XŒ≤^(t) + W‚Åª¬π(y - p^(t)) is the "working response"</p>
                </div>
            </div>

            <div class="section">
                <h2>üìä Inference and Hypothesis Testing</h2>

                <div class="theorem-box">
                    <h4>üéØ Asymptotic Distribution</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        ‚àön(Œ≤ÃÇ - Œ≤) ‚áí N(0, I‚Åª¬π(Œ≤))
                    </div>
                    <p>where I(Œ≤) is the Fisher information matrix</p>
                </div>

                <div class="grid-3">
                    <div class="logistic-concept">
                        <h4>üìä Wald Tests</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            z = Œ≤ÃÇ‚±º/se(Œ≤ÃÇ‚±º) ~ N(0,1)
                        </div>
                        <p><strong>For individual parameters</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>üéØ Likelihood Ratio Tests</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            LRT = 2[‚Ñì(Œ≤ÃÇ) - ‚Ñì(Œ≤ÃÇ‚ÇÄ)] ~ œá¬≤_q
                        </div>
                        <p><strong>For nested models</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>üìà Score Tests</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            LM = S(Œ≤ÃÇ‚ÇÄ)'I‚Åª¬π(Œ≤ÃÇ‚ÇÄ)S(Œ≤ÃÇ‚ÇÄ) ~ œá¬≤_q
                        </div>
                        <p><strong>Only need null model</strong></p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üîç Confidence Intervals</h4>
                    <p><strong>For coefficients:</strong> Œ≤ÃÇ‚±º ¬± z_{Œ±/2} √ó se(Œ≤ÃÇ‚±º)</p>
                    <p><strong>For odds ratios:</strong> exp(Œ≤ÃÇ‚±º ¬± z_{Œ±/2} √ó se(Œ≤ÃÇ‚±º))</p>
                    <p><strong>Profile likelihood intervals:</strong> Often more accurate for small samples</p>
                </div>
            </div>

            <div class="section">
                <h2>ü©∫ Model Diagnostics</h2>

                <div class="diagnostic-box">
                    <h4>üîç Goodness-of-Fit Assessment</h4>
                    <p>How well does our model fit the data?</p>
                </div>

                <div class="grid-2">
                    <div class="logistic-concept">
                        <h4>üìä Deviance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            D = 2[‚Ñì_saturated - ‚Ñì(Œ≤ÃÇ)]
                        </div>
                        <p><strong>Measures:</strong> Distance from perfect fit</p>
                    </div>

                    <div class="logistic-concept">
                        <h4>üéØ Pearson Chi-square</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                            X¬≤ = Œ£·µ¢ (y·µ¢ - pÃÇ·µ¢)¬≤ / (pÃÇ·µ¢(1-pÃÇ·µ¢))
                        </div>
                        <p><strong>Alternative:</strong> Goodness-of-fit measure</p>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>üè• Hosmer-Lemeshow Test</h4>
                    <p><strong>Procedure:</strong></p>
                    <ol>
                        <li>Group observations by predicted probabilities</li>
                        <li>Compare observed vs expected in each group</li>
                        <li>Test statistic follows œá¬≤ distribution</li>
                    </ol>
                    <p><strong>H‚ÇÄ:</strong> Model fits well (want large p-value!)</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>üìä Residuals for Binary Data</h4>
                        <p><strong>Pearson residuals:</strong></p>
                        <div class="formula-box">
                            r·µ¢·¥æ = (y·µ¢ - pÃÇ·µ¢)/‚àö(pÃÇ·µ¢(1-pÃÇ·µ¢))
                        </div>
                        <p><strong>Deviance residuals:</strong></p>
                        <div class="formula-box">
                            r·µ¢·¥∞ = sign(y·µ¢ - pÃÇ·µ¢)‚àö[2y·µ¢log(y·µ¢/pÃÇ·µ¢) + 2(1-y·µ¢)log((1-y·µ¢)/(1-pÃÇ·µ¢))]
                        </div>
                    </div>

                    <div class="example-box">
                        <h4>üéØ Influential Observations</h4>
                        <p><strong>Cook's distance:</strong></p>
                        <div class="formula-box">
                            D·µ¢ = (r·µ¢·¥æ)¬≤/(p+1) √ó h·µ¢·µ¢/(1-h·µ¢·µ¢)
                        </div>
                        <p><strong>DFBETAS:</strong> Change in Œ≤ÃÇ when removing observation i</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üìà ROC Curves and Classification</h2>

                <div class="theorem-box">
                    <h4>üìä Receiver Operating Characteristic (ROC)</h4>
                    <p><strong>ROC curve:</strong> Plot of Sensitivity vs (1 - Specificity)</p>
                    <ul>
                        <li><strong>Sensitivity:</strong> P(predict positive | actually positive) = TP/(TP+FN)</li>
                        <li><strong>Specificity:</strong> P(predict negative | actually negative) = TN/(TN+FP)</li>
                    </ul>
                </div>

                <div class="sigmoid-visual">
                    <h4>üéØ Classification Metrics</h4>
                    <table class="comparison-table" style="margin: 20px auto; max-width: 500px;">
                        <tr>
                            <th></th>
                            <th>Predicted +</th>
                            <th>Predicted -</th>
                        </tr>
                        <tr>
                            <td><strong>Actual +</strong></td>
                            <td>TP (True Positive)</td>
                            <td>FN (False Negative)</td>
                        </tr>
                        <tr>
                            <td><strong>Actual -</strong></td>
                            <td>FP (False Positive)</td>
                            <td>TN (True Negative)</td>
                        </tr>
                    </table>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>üìè Area Under Curve (AUC)</h4>
                        <ul>
                            <li><strong>AUC = 0.5:</strong> No discriminatory ability (random)</li>
                            <li><strong>AUC = 0.7:</strong> Acceptable discrimination</li>
                            <li><strong>AUC = 0.8:</strong> Excellent discrimination</li>
                            <li><strong>AUC = 1.0:</strong> Perfect discrimination</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>‚öñÔ∏è Choosing Classification Threshold</h4>
                        <p><strong>Default:</strong> pÃÇ = 0.5</p>
                        <p><strong>Medical testing:</strong> Lower threshold (don't miss disease)</p>
                        <p><strong>Spam detection:</strong> Higher threshold (don't block real email)</p>
                        <p><strong>Optimal:</strong> Maximize (Sensitivity + Specificity)</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üèóÔ∏è Extensions and Variations</h2>

                <div class="theorem-box">
                    <h4>üéØ Multinomial Logistic Regression</h4>
                    <p>For outcomes with K > 2 categories:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        P(Y = j|x) = exp(x'Œ≤‚±º) / Œ£‚Çñ‚Çå‚ÇÅ·¥∑ exp(x'Œ≤‚Çñ)
                    </div>
                    <p><strong>Constraint:</strong> Set Œ≤‚ÇÅ = 0 (reference category)</p>
                </div>

                <div class="grid-3">
                    <div class="logistic-concept">
                        <h4>üìä Ordinal Logistic</h4>
                        <p><strong>For ordered categories</strong></p>
                        <p>Examples: Likert scales, severity ratings</p>
                        <p><strong>Uses cumulative probabilities</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>üéØ Mixed Effects</h4>
                        <p><strong>For clustered/repeated data</strong></p>
                        <p>Examples: Students within schools</p>
                        <p><strong>Accounts for correlation</strong></p>
                    </div>

                    <div class="logistic-concept">
                        <h4>üìà Regularized Logistic</h4>
                        <p><strong>Ridge:</strong> L2 penalty</p>
                        <p><strong>LASSO:</strong> L1 penalty</p>
                        <p><strong>Elastic Net:</strong> L1 + L2</p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üåü Generalized Linear Models (GLMs)</h4>
                    <p>Logistic regression is a special case of GLMs!</p>
                    <p><strong>Components:</strong></p>
                    <ul>
                        <li><strong>Random component:</strong> Y ~ Binomial</li>
                        <li><strong>Systematic component:</strong> Œ∑ = XŒ≤</li>
                        <li><strong>Link function:</strong> logit(Œº) = Œ∑</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>‚ö†Ô∏è Common Pitfalls and Solutions</h2>

                <div class="grid-2">
                    <div class="problem-box">
                        <h4>‚ùå Complete Separation</h4>
                        <p><strong>Problem:</strong> Perfect classification by one variable</p>
                        <p><strong>Result:</strong> Infinite coefficient estimates</p>
                        <p><strong>Solution:</strong> Penalized likelihood, Firth regression</p>
                    </div>

                    <div class="problem-box">
                        <h4>‚ùå Quasi-Complete Separation</h4>
                        <p><strong>Problem:</strong> Nearly perfect separation</p>
                        <p><strong>Result:</strong> Very large standard errors</p>
                        <p><strong>Detection:</strong> Extremely large coefficients</p>
                    </div>
                </div>

                <div class="problem-box">
                    <h4>‚ùå Misinterpreting Coefficients</h4>
                    <p><strong>Problem:</strong> Thinking Œ≤ represents change in probability</p>
                    <p><strong>Reality:</strong> Œ≤ represents change in log-odds</p>
                    <p><strong>Solution:</strong> Report odds ratios: OR = e^Œ≤</p>
                </div>

                <div class="grid-2">
                    <div class="problem-box">
                        <h4>‚ùå Ignoring Non-linearity</h4>
                        <p><strong>Problem:</strong> Assuming linear relationship in logit</p>
                        <p><strong>Solution:</strong> Add polynomial terms, splines, or interactions</p>
                    </div>

                    <div class="problem-box">
                        <h4>‚ùå Small Sample Issues</h4>
                        <p><strong>Problem:</strong> Asymptotic theory breaks down</p>
                        <p><strong>Rule of thumb:</strong> Need 10+ events per predictor</p>
                        <p><strong>Solution:</strong> Exact methods, penalized likelihood</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üéØ Model Selection and Regularization</h2>

                <div class="theorem-box">
                    <h4>üìä Information Criteria</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        AIC = -2‚Ñì(Œ≤ÃÇ) + 2p
                        <br>BIC = -2‚Ñì(Œ≤ÃÇ) + p log(n)
                    </div>
                    <p><strong>Lower is better!</strong> Balance fit and complexity</p>
                </div>

                <div class="mle-visual">
                    <h2>üîß Regularized Logistic Regression</h2>
                    <p>Add penalties to prevent overfitting in high dimensions!</p>
                </div>

                <div class="iteration-steps">
                    <div class="iteration-step">
                        <h4>üîµ Ridge Logistic</h4>
                        <div class="formula-box">
                            Œ≤ÃÇ·¥ø‚Å±·µà·µç·µâ = argmin{-‚Ñì(Œ≤) + ŒªŒ£Œ≤‚±º¬≤}
                        </div>
                        <p><strong>Shrinks coefficients</strong></p>
                    </div>

                    <div class="iteration-step">
                        <h4>üî∂ LASSO Logistic</h4>
                        <div class="formula-box">
                            Œ≤ÃÇ·¥∏·¥¨À¢À¢·¥º = argmin{-‚Ñì(Œ≤) + ŒªŒ£|Œ≤‚±º|}
                        </div>
                        <p><strong>Variable selection</strong></p>
                    </div>

                    <div class="iteration-step">
                        <h4>üü£ Elastic Net</h4>
                        <div class="formula-box">
                            Œ≤ÃÇ·¥±·¥∫ = argmin{-‚Ñì(Œ≤) + Œª‚ÇÅŒ£|Œ≤‚±º| + Œª‚ÇÇŒ£Œ≤‚±º¬≤}
                        </div>
                        <p><strong>Best of both</strong></p>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üîç Cross-Validation for Logistic Regression</h4>
                    <p><strong>k-fold CV:</strong> Split data, train on k-1 folds, test on 1</p>
                    <p><strong>Metrics to optimize:</strong></p>
                    <ul>
                        <li><strong>Log-likelihood:</strong> Probabilistic scoring</li>
                        <li><strong>Classification accuracy:</strong> Percent correct</li>
                        <li><strong>AUC:</strong> Area under ROC curve</li>
                        <li><strong>F1-score:</strong> Harmonic mean of precision and recall</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>üîó Connections to Other Chapters</h2>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>üìä To Linear Regression (Chapter 14)</h4>
                        <ul>
                            <li><strong>Extension:</strong> From continuous to binary outcomes</li>
                            <li><strong>Similar concepts:</strong> Coefficients, inference, diagnostics</li>
                            <li><strong>Key difference:</strong> Link function and distribution</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üìà To Convergence (Chapter 6)</h4>
                        <ul>
                            <li><strong>Asymptotic normality:</strong> ‚àön(Œ≤ÃÇ - Œ≤) ‚áí N(0,V)</li>
                            <li><strong>Consistency:</strong> Œ≤ÃÇ‚Çô ‚Üí·µñ Œ≤</li>
                            <li><strong>Central limit theorem:</strong> Foundation for inference</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üî¨ To Hypothesis Testing (Chapter 11)</h4>
                        <ul>
                            <li><strong>Wald tests:</strong> Individual coefficient tests</li>
                            <li><strong>Likelihood ratio tests:</strong> Nested model comparison</li>
                            <li><strong>Score tests:</strong> Efficient testing procedures</li>
                        </ul>
                    </div>
                </div>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>üìä To Nonparametric Methods (Chapter 17)</h4>
                        <ul>
                            <li><strong>GAMs:</strong> Smooth functions in logistic models</li>
                            <li><strong>Classification trees:</strong> Alternative to logistic regression</li>
                            <li><strong>Kernel methods:</strong> Nonparametric classification</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>ü§ñ To Machine Learning (Chapter 23)</h4>
                        <ul>
                            <li><strong>Classification:</strong> Logistic regression as ML method</li>
                            <li><strong>Regularization:</strong> Ridge, LASSO, Elastic Net</li>
                            <li><strong>Cross-validation:</strong> Model selection and evaluation</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üìä To High-Dimensional (Chapter 21)</h4>
                        <ul>
                            <li><strong>Sparse models:</strong> LASSO for variable selection</li>
                            <li><strong>Regularization theory:</strong> Oracle properties</li>
                            <li><strong>Computational methods:</strong> Coordinate descent</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üíº Real-World Applications</h2>

                <div class="grid-2">
                    <div class="diagnostic-box" style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%);">
                        <h4>üè• Medical Applications</h4>
                        <ul>
                            <li><strong>Disease diagnosis:</strong> Predict disease from symptoms</li>
                            <li><strong>Treatment response:</strong> Will patient respond to therapy?</li>
                            <li><strong>Risk scoring:</strong> Probability of adverse events</li>
                            <li><strong>Clinical trials:</strong> Binary endpoints</li>
                        </ul>
                    </div>

                    <div class="diagnostic-box" style="background: linear-gradient(135deg, #4ecdc4 0%, #44a08d 100%);">
                        <h4>üí∞ Business Applications</h4>
                        <ul>
                            <li><strong>Marketing:</strong> Will customer buy product?</li>
                            <li><strong>Credit scoring:</strong> Will borrower default?</li>
                            <li><strong>Churn prediction:</strong> Will customer leave?</li>
                            <li><strong>A/B testing:</strong> Did intervention work?</li>
                        </ul>
                    </div>
                </div>

                <div class="example-box">
                    <h4>üéØ Example: Email Spam Detection</h4>
                    <p><strong>Outcome:</strong> Y = 1 if spam, 0 if legitimate</p>
                    <p><strong>Predictors:</strong> Word frequencies, sender info, etc.</p>
                    <p><strong>Model:</strong> logit(P(spam)) = Œ≤‚ÇÄ + Œ≤‚ÇÅ(frequency of "free") + Œ≤‚ÇÇ(ALL CAPS) + ...</p>
                    <p><strong>Interpretation:</strong> OR > 1 means higher spam probability</p>
                    <p><strong>Decision rule:</strong> Classify as spam if pÃÇ > threshold</p>
                </div>
            </div>

            <div class="section">
                <h2>üåü Summary and Key Takeaways</h2>

                <div class="diagnostic-box">
                    <h4>üéØ Essential Concepts</h4>
                    <ul>
                        <li><strong>Logistic function:</strong> Maps real line to (0,1) for probabilities</li>
                        <li><strong>Logit transformation:</strong> Makes relationship linear in parameters</li>
                        <li><strong>Maximum likelihood:</strong> No closed form, need iterative methods</li>
                        <li><strong>Odds ratios:</strong> Natural interpretation of coefficients</li>
                        <li><strong>ROC curves:</strong> Evaluate classification performance</li>
                        <li><strong>Regularization:</strong> Prevents overfitting in high dimensions</li>
                    </ul>
                </div>

                <div class="sigmoid-visual">
                    <h4>üöÄ The Big Picture</h4>
                    <p>Logistic regression extends linear modeling to <strong>binary outcomes</strong>. Key advantages:</p>
                    <ul style="text-align: left; max-width: 600px; margin: 0 auto;">
                        <li>Outputs interpretable probabilities</li>
                        <li>No distributional assumptions on predictors</li>
                        <li>Robust and widely applicable</li>
                        <li>Foundation for many machine learning methods</li>
                        <li>Extends naturally to multiple categories</li>
                    </ul>
                </div>

                <div class="theorem-box">
                    <h4>üéì Best Practices</h4>
                    <ul>
                        <li><strong>Check for separation:</strong> Inspect coefficient magnitudes</li>
                        <li><strong>Assess fit:</strong> Use Hosmer-Lemeshow test, ROC curves</li>
                        <li><strong>Validate predictions:</strong> Cross-validation essential</li>
                        <li><strong>Report odds ratios:</strong> More interpretable than coefficients</li>
                        <li><strong>Consider regularization:</strong> Especially with many predictors</li>
                        <li><strong>Choose threshold wisely:</strong> Consider costs of false positives/negatives</li>
                    </ul>
                </div>

                <div class="example-box">
                    <h4>üîÆ Advanced Topics to Explore</h4>
                    <ul>
                        <li><strong>Bayesian logistic regression:</strong> Incorporates prior information</li>
                        <li><strong>Exact logistic regression:</strong> For small samples</li>
                        <li><strong>Robust logistic regression:</strong> Handles outliers</li>
                        <li><strong>Nonlinear logistic models:</strong> GAMs, neural networks</li>
                        <li><strong>Multilevel models:</strong> Hierarchical/clustered data</li>
                    </ul>
                </div>

                <div class="mle-visual">
                    <h2>üí° Why Logistic Regression Matters</h2>
                    <p>In our data-driven world, many decisions are binary: approve/deny, buy/don't buy, sick/healthy. Logistic regression provides a <strong>principled statistical framework</strong> for making these decisions based on data, with proper uncertainty quantification and interpretable results.</p>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="#" class="nav-btn">‚Üê Chapter 15: Multiple Regression</a>
            <a href="#" class="nav-btn">Chapter 17: Nonparametric Smoothing ‚Üí</a>
        </div>
    </div>
</body>
</html>