<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 23: Classification</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: 'üìäüéØüìàüéØüìä';
            position: absolute;
            top: 50%;
            left: -20%;
            transform: translateY(-50%);
            font-size: 6em;
            opacity: 0.1;
            animation: scroll 15s linear infinite;
        }

        @keyframes scroll {
            0% { left: -20%; }
            100% { left: 120%; }
        }

        .header h1 {
            position: relative;
            z-index: 1;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header p {
            position: relative;
            z-index: 1;
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 40px;
            padding: 30px;
            border-radius: 15px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            border-left: 5px solid #667eea;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .section::before {
            content: '';
            position: absolute;
            top: -100%;
            right: -100%;
            width: 200px;
            height: 200px;
            background: radial-gradient(circle, rgba(102,126,234,0.1) 0%, transparent 70%);
            border-radius: 50%;
            animation: pulse 4s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); opacity: 0.3; }
            50% { transform: scale(1.2); opacity: 0.1; }
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.2);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            display: inline-block;
            position: relative;
            z-index: 1;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            position: relative;
            z-index: 1;
        }

        .classifier-type {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.3);
            transition: all 0.3s ease;
            position: relative;
        }

        .classifier-type::before {
            content: 'üéØ';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 3em;
            opacity: 0.3;
        }

        .classifier-type:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.4);
        }

        .classifier-type h4 {
            font-size: 1.4em;
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }

        .formula-box {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #f39c12;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            box-shadow: 0 8px 20px rgba(243, 156, 18, 0.2);
            position: relative;
            overflow: hidden;
        }

        .formula-box::before {
            content: 'P(Y|X)';
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 1.5em;
            opacity: 0.2;
            color: #d35400;
            font-style: italic;
        }

        .example-box {
            background: linear-gradient(135deg, #e0c3fc 0%, #9bb5ff 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #9c88ff;
            box-shadow: 0 8px 20px rgba(156, 136, 255, 0.2);
            position: relative;
        }

        .example-box h4 {
            color: #5a4fcf;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .theorem-box {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(255, 107, 107, 0.3);
            position: relative;
        }

        .theorem-box::before {
            content: '‚àÄx';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 1.5em;
            opacity: 0.7;
            font-style: italic;
        }

        .theorem-box h4 {
            font-size: 1.3em;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        .highlight {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: bold;
            color: #2c3e50;
            display: inline-block;
            margin: 2px;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin: 20px 0;
        }

        .comparison-table th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        .comparison-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }

        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .visual-demo {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            border: 3px solid #f39c12;
            text-align: center;
        }

        .sequence-demo {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 15px 0;
            font-size: 1.2em;
            font-weight: bold;
        }

        .sequence-demo .arrow {
            color: #e67e22;
            font-size: 2em;
        }

        .algorithm-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.3);
        }

        .algorithm-box h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
            text-align: center;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 15px;
        }

        .pitfall-box {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);
            color: white;
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            box-shadow: 0 8px 20px rgba(255, 107, 107, 0.3);
        }

        .pitfall-box h4 {
            font-size: 1.2em;
            margin-bottom: 10px;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .metric-card {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #667eea;
            text-align: center;
        }

        .metric-card h5 {
            color: #2c3e50;
            font-size: 1.1em;
            margin-bottom: 10px;
        }

        ul, ol {
            margin-left: 25px;
            margin-top: 10px;
        }

        li {
            margin-bottom: 8px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        }

        .nav-btn {
            padding: 12px 25px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.4);
        }

        @media (max-width: 768px) {
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .content {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chapter 23: Classification</h1>
            <p>Predicting Discrete Responses from Data</p>
        </div>

        <div class="content">
            <div class="section">
                <h2>üéØ Introduction to Classification</h2>
                <p>Classification is the problem of predicting a discrete variable Y from another random variable X. We have IID data (X‚ÇÅ, Y‚ÇÅ), ..., (X‚Çô, Y‚Çô) where X ‚àà ùí≥ ‚äÇ ‚Ñù·µà and Y ‚àà ùí¥ = {1, 2, ..., k}.</p>
                
                <div class="example-box">
                    <h4>üåü Real-World Examples</h4>
                    <ul>
                        <li><strong>Email filtering:</strong> Spam vs. Not spam</li>
                        <li><strong>Medical diagnosis:</strong> Disease vs. Healthy</li>
                        <li><strong>Image recognition:</strong> Cat, Dog, Bird classifications</li>
                        <li><strong>Credit scoring:</strong> Good, Bad credit risk</li>
                        <li><strong>Marketing:</strong> Customer segments</li>
                    </ul>
                </div>

                <div class="visual-demo">
                    <h4>üîç Classification vs. Regression</h4>
                    <table class="comparison-table">
                        <tr>
                            <th>Aspect</th>
                            <th>Classification</th>
                            <th>Regression</th>
                        </tr>
                        <tr>
                            <td><strong>Response</strong></td>
                            <td>Discrete categories</td>
                            <td>Continuous values</td>
                        </tr>
                        <tr>
                            <td><strong>Goal</strong></td>
                            <td>Predict class membership</td>
                            <td>Predict numeric value</td>
                        </tr>
                        <tr>
                            <td><strong>Loss function</strong></td>
                            <td>0-1 loss, log-loss</td>
                            <td>Squared error</td>
                        </tr>
                        <tr>
                            <td><strong>Output</strong></td>
                            <td>Class probabilities</td>
                            <td>Point estimates</td>
                        </tr>
                    </table>
                </div>
            </div>

            <div class="section">
                <h2>‚öñÔ∏è Risk and the Bayes Classifier</h2>

                <div class="theorem-box">
                    <h4>üìä Risk Function</h4>
                    <p>The <strong>risk</strong> (misclassification error) of classifier h is:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        R(h) = P(h(X) ‚â† Y) = E[I(h(X) ‚â† Y)]
                    </div>
                </div>

                <div class="algorithm-box">
                    <h4>üéØ Bayes Classifier</h4>
                    <p>The optimal classifier that minimizes risk:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        h*(x) = argmax P(Y = j|X = x)
                                  j
                    </div>
                    <p><strong>Bayes risk:</strong> R* = R(h*) (minimum possible risk)</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>üîç Posterior Probabilities</h4>
                        <p>Let Œ∑_j(x) = P(Y = j|X = x)</p>
                        <p><strong>Properties:</strong></p>
                        <ul>
                            <li>‚àë_j Œ∑_j(x) = 1</li>
                            <li>Œ∑_j(x) ‚â• 0</li>
                            <li>h*(x) = argmax Œ∑_j(x)</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üé≤ Binary Classification</h4>
                        <p>For k = 2 classes:</p>
                        <div class="formula-box">
                            h*(x) = {1 if Œ∑‚ÇÅ(x) > 1/2
                                    {0 if Œ∑‚ÇÅ(x) < 1/2
                        </div>
                        <p><strong>Decision boundary:</strong> {x : Œ∑‚ÇÅ(x) = 1/2}</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üìè Linear and Quadratic Discriminant Analysis</h2>

                <div class="classifier-type">
                    <h4>üìä Linear Discriminant Analysis (LDA)</h4>
                    <p><strong>Assumptions:</strong></p>
                    <ul>
                        <li>X|Y = j ~ N(Œº‚±º, Œ£) (same covariance)</li>
                        <li>P(Y = j) = œÄ‚±º</li>
                    </ul>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        Discriminant functions:
                        <br>Œ¥‚±º(x) = log œÄ‚±º - (1/2)Œº‚±º·µÄŒ£‚Åª¬πŒº‚±º + x·µÄŒ£‚Åª¬πŒº‚±º
                    </div>
                    <p><strong>Classification:</strong> h(x) = argmax Œ¥‚±º(x)</p>
                </div>

                <div class="classifier-type">
                    <h4>üìà Quadratic Discriminant Analysis (QDA)</h4>
                    <p><strong>Assumption:</strong> X|Y = j ~ N(Œº‚±º, Œ£‚±º) (different covariances)</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        Œ¥‚±º(x) = log œÄ‚±º - (1/2)log|Œ£‚±º| - (1/2)(x - Œº‚±º)·µÄŒ£‚±º‚Åª¬π(x - Œº‚±º)
                    </div>
                    <p><strong>Decision boundaries:</strong> Quadratic in x</p>
                </div>

                <div class="visual-demo">
                    <h4>‚öñÔ∏è LDA vs QDA Trade-off</h4>
                    <div class="grid-2">
                        <div style="background: rgba(102, 126, 234, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>üìä LDA</h5>
                            <p><strong>Lower variance</strong></p>
                            <p><strong>Higher bias</strong></p>
                            <p>Assumes equal covariances</p>
                        </div>
                        <div style="background: rgba(255, 107, 107, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>üìà QDA</h5>
                            <p><strong>Higher variance</strong></p>
                            <p><strong>Lower bias</strong></p>
                            <p>More flexible model</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üìä Logistic Regression</h2>

                <div class="algorithm-box">
                    <h4>üéØ Binary Logistic Model</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        log(P(Y = 1|X = x)/(P(Y = 0|X = x))) = Œ≤‚ÇÄ + Œ≤·µÄx
                    </div>
                    <p><strong>Probability:</strong></p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        P(Y = 1|X = x) = exp(Œ≤‚ÇÄ + Œ≤·µÄx)/(1 + exp(Œ≤‚ÇÄ + Œ≤·µÄx))
                    </div>
                </div>

                <div class="classifier-type">
                    <h4>üî¢ Multinomial Logistic Regression</h4>
                    <p>For k classes, use k-1 log-odds:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        log(P(Y = j|X = x)/P(Y = k|X = x)) = Œ≤‚ÇÄ‚±º + Œ≤‚±º·µÄx
                        <br>for j = 1, ..., k-1
                    </div>
                    <p><strong>No closed form solution:</strong> Use iterative methods (Newton-Raphson, IRLS)</p>
                </div>

                <div class="example-box">
                    <h4>üìà Maximum Likelihood Estimation</h4>
                    <div class="formula-box">
                        ‚Ñì(Œ≤) = ‚àë·µ¢‚Çå‚ÇÅ‚Åø ‚àë‚±º‚Çå‚ÇÅ·µè Y·µ¢‚±º log P(Y = j|X·µ¢, Œ≤)
                    </div>
                    <p>where Y·µ¢‚±º = I(Y·µ¢ = j)</p>
                    <p><strong>Advantages:</strong> Flexible, interpretable coefficients, probabilistic output</p>
                </div>
            </div>

            <div class="section">
                <h2>üå≥ Classification Trees</h2>

                <div class="algorithm-box">
                    <h4>üîÑ Recursive Binary Splitting</h4>
                    <ol>
                        <li>Find best split (feature + threshold)</li>
                        <li>Split data into two subsets</li>
                        <li>Repeat recursively on each subset</li>
                        <li>Stop when stopping criterion met</li>
                    </ol>
                </div>

                <div class="grid-3">
                    <div class="classifier-type">
                        <h4>üìä Misclassification Rate</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ‚àë‚±º I(ƒµ ‚â† j)pÃÇ‚±º
                        </div>
                        <p>Simple but not differentiable</p>
                    </div>

                    <div class="classifier-type">
                        <h4>üéØ Gini Index</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ‚àë‚±º pÃÇ‚±º(1 - pÃÇ‚±º) = 1 - ‚àë‚±º pÃÇ‚±º¬≤
                        </div>
                        <p>Measures node "impurity"</p>
                    </div>

                    <div class="classifier-type">
                        <h4>üìà Entropy</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            -‚àë‚±º pÃÇ‚±º log pÃÇ‚±º
                        </div>
                        <p>Information-theoretic measure</p>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>‚úÇÔ∏è Cost-Complexity Pruning</h4>
                    <p>Large trees overfit. Use cost-complexity criterion:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        CŒ±(T) = ‚àë‚Çú N‚Çú Q‚Çú + Œ±|T|
                    </div>
                    <p>Where N‚Çú = observations in leaf t, Q‚Çú = misclassification rate, |T| = number of leaves</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>‚úÖ Tree Advantages</h4>
                        <ul>
                            <li>Easy to interpret</li>
                            <li>Handles mixed data types</li>
                            <li>Automatic feature selection</li>
                            <li>Captures interactions</li>
                            <li>No distributional assumptions</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>‚ö†Ô∏è Tree Disadvantages</h4>
                        <ul>
                            <li>High variance</li>
                            <li>Biased toward variables with more levels</li>
                            <li>Difficulty capturing linear relationships</li>
                            <li>Greedy algorithm (not globally optimal)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üéØ k-Nearest Neighbors (k-NN)</h2>

                <div class="algorithm-box">
                    <h4>üîç k-NN Algorithm</h4>
                    <ol>
                        <li>Find k nearest neighbors to x in training set</li>
                        <li>Classify x as majority class among k neighbors</li>
                    </ol>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        ƒ•(x) = argmax ‚àë·µ¢‚ààN‚Çñ(x) I(y·µ¢ = j)
                               j
                    </div>
                </div>

                <div class="grid-3">
                    <div class="classifier-type">
                        <h4>üìè Euclidean Distance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ||x - x·µ¢||‚ÇÇ = ‚àö‚àë‚±º(x‚±º - x·µ¢‚±º)¬≤
                        </div>
                    </div>

                    <div class="classifier-type">
                        <h4>üö∂ Manhattan Distance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ||x - x·µ¢||‚ÇÅ = ‚àë‚±º|x‚±º - x·µ¢‚±º|
                        </div>
                    </div>

                    <div class="classifier-type">
                        <h4>‚ö° Minkowski Distance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ||x - x·µ¢||‚Çö = (‚àë‚±º|x‚±º - x·µ¢‚±º|·µñ)^(1/p)
                        </div>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>üìä Theoretical Properties</h4>
                    <p><strong>Consistency:</strong> As n ‚Üí ‚àû and k ‚Üí ‚àû with k/n ‚Üí 0:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        R(ƒ•‚Çñ) ‚Üí R* (Bayes risk)
                    </div>
                    <p><strong>Curse of dimensionality:</strong> Performance degrades in high dimensions</p>
                </div>

                <div class="visual-demo">
                    <h4>üéõÔ∏è Choosing k</h4>
                    <div class="grid-3">
                        <div style="background: rgba(255, 107, 107, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>Small k</h5>
                            <p>Low bias</p>
                            <p>High variance</p>
                        </div>
                        <div style="background: rgba(107, 207, 127, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>k = ‚àön</h5>
                            <p>Common choice</p>
                            <p>Good balance</p>
                        </div>
                        <div style="background: rgba(102, 126, 234, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>Large k</h5>
                            <p>High bias</p>
                            <p>Low variance</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>‚ö° Support Vector Machines (SVM)</h2>

                <div class="algorithm-box">
                    <h4>üìè Linear SVM (Separable Case)</h4>
                    <p><strong>Goal:</strong> Find hyperplane with maximum margin</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        minimize (1/2)||w||¬≤
                        subject to y·µ¢(w·µÄx·µ¢ + b) ‚â• 1, i = 1, ..., n
                    </div>
                    <p><strong>Margin:</strong> 1/||w|| (distance to nearest point)</p>
                </div>

                <div class="classifier-type">
                    <h4>üîß Soft Margin SVM</h4>
                    <p>Allow some misclassification with slack variables:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        minimize (1/2)||w||¬≤ + C‚àë·µ¢Œæ·µ¢
                        subject to y·µ¢(w·µÄx·µ¢ + b) ‚â• 1 - Œæ·µ¢, Œæ·µ¢ ‚â• 0
                    </div>
                    <p><strong>C parameter:</strong> Controls trade-off between margin and violations</p>
                </div>

                <div class="theorem-box">
                    <h4>üéØ Kernel Trick</h4>
                    <p>Transform to higher-dimensional space using kernel functions:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        K(x, x') = œÜ(x)·µÄœÜ(x')
                    </div>
                    <p><strong>Decision function:</strong></p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        f(x) = ‚àë·µ¢ Œ±·µ¢y·µ¢K(x·µ¢, x) + b
                    </div>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>üîß Common Kernels</h4>
                        <ul>
                            <li><strong>Linear:</strong> K(x, x') = x·µÄx'</li>
                            <li><strong>Polynomial:</strong> K(x, x') = (Œ≥x·µÄx' + r)^d</li>
                            <li><strong>RBF:</strong> K(x, x') = exp(-Œ≥||x - x'||¬≤)</li>
                            <li><strong>Sigmoid:</strong> K(x, x') = tanh(Œ≥x·µÄx' + r)</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>‚≠ê SVM Properties</h4>
                        <ul>
                            <li><strong>Maximum margin:</strong> Unique solution</li>
                            <li><strong>Sparse:</strong> Only support vectors matter</li>
                            <li><strong>Kernel trick:</strong> Handles nonlinear boundaries</li>
                            <li><strong>Regularization:</strong> C controls overfitting</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üß† Neural Networks</h2>

                <div class="classifier-type">
                    <h4>‚ö° Single Layer Perceptron</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        f(x) = œÉ(w‚ÇÄ + ‚àë‚±ºw‚±ºx‚±º)
                    </div>
                    <p>Where œÉ is activation function (sigmoid, tanh, ReLU)</p>
                </div>

                <div class="algorithm-box">
                    <h4>üèóÔ∏è Multi-Layer Perceptron</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        Hidden layer: h = œÉ(W‚ÇÅx + b‚ÇÅ)
                        <br>Output layer: y = œÉ(W‚ÇÇh + b‚ÇÇ)
                    </div>
                    <p><strong>Training:</strong> Backpropagation algorithm</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>‚úÖ Neural Network Advantages</h4>
                        <ul>
                            <li>Universal approximation capability</li>
                            <li>Automatic feature learning</li>
                            <li>Handles complex nonlinear patterns</li>
                            <li>Scalable to large datasets</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>‚ö†Ô∏è Neural Network Challenges</h4>
                        <ul>
                            <li>Many hyperparameters to tune</li>
                            <li>Prone to overfitting</li>
                            <li>Black box (low interpretability)</li>
                            <li>Requires large amounts of data</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üìä Model Evaluation and Selection</h2>

                <div class="algorithm-box">
                    <h4>üîÑ Cross-Validation</h4>
                    <ol>
                        <li>Divide data into k folds</li>
                        <li>For each fold: train on k-1 folds, test on remaining</li>
                        <li>Average test errors</li>
                    </ol>
                    <p><strong>Leave-one-out CV:</strong> Special case with k = n</p>
                </div>

                <div class="visual-demo">
                    <h4>üìà Confusion Matrix</h4>
                    <table class="comparison-table">
                        <tr>
                            <th></th>
                            <th colspan="2">Predicted</th>
                        </tr>
                        <tr>
                            <th>Actual</th>
                            <th>Positive</th>
                            <th>Negative</th>
                        </tr>
                        <tr>
                            <td><strong>Positive</strong></td>
                            <td style="background: #d4edda;">TP</td>
                            <td style="background: #f8d7da;">FN</td>
                        </tr>
                        <tr>
                            <td><strong>Negative</strong></td>
                            <td style="background: #f8d7da;">FP</td>
                            <td style="background: #d4edda;">TN</td>
                        </tr>
                    </table>
                </div>

                <div class="metrics-grid">
                    <div class="metric-card">
                        <h5>üéØ Accuracy</h5>
                        <div class="formula-box">
                            (TP + TN)/(TP + TN + FP + FN)
                        </div>
                    </div>
                    <div class="metric-card">
                        <h5>üîç Precision</h5>
                        <div class="formula-box">
                            TP/(TP + FP)
                        </div>
                    </div>
                    <div class="metric-card">
                        <h5>üì° Recall</h5>
                        <div class="formula-box">
                            TP/(TP + FN)
                        </div>
                    </div>
                    <div class="metric-card">
                        <h5>‚öñÔ∏è F1-Score</h5>
                        <div class="formula-box">
                            2 √ó (Precision √ó Recall)/(Precision + Recall)
                        </div>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>üìà ROC and AUC</h4>
                    <p><strong>ROC curve:</strong> Plot of True Positive Rate vs False Positive Rate</p>
                    <p><strong>AUC:</strong> Area Under the ROC Curve</p>
                    <ul>
                        <li>AUC = 0.5: Random classifier</li>
                        <li>AUC = 1.0: Perfect classifier</li>
                        <li>Higher AUC = Better performance</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>‚ö†Ô∏è Common Pitfalls and Practical Considerations</h2>

                <div class="grid-2">
                    <div class="pitfall-box">
                        <h4>‚ùå Data Leakage</h4>
                        <p><strong>Problem:</strong> Information from future/target leaks into features</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Careful feature engineering</li>
                            <li>Temporal validation for time series</li>
                            <li>Strict train/validation/test splits</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>‚öñÔ∏è Imbalanced Classes</h4>
                        <p><strong>Problem:</strong> Some classes much more frequent</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Resampling (SMOTE, undersampling)</li>
                            <li>Cost-sensitive learning</li>
                            <li>Different evaluation metrics</li>
                        </ul>
                    </div>
                </div>

                <div class="grid-2">
                    <div class="pitfall-box">
                        <h4>üéØ Overfitting</h4>
                        <p><strong>Symptoms:</strong> High training accuracy, poor test performance</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Regularization (L1, L2)</li>
                            <li>Cross-validation</li>
                            <li>More data</li>
                            <li>Simpler models</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>üìè Feature Scaling</h4>
                        <p><strong>Problem:</strong> Features on different scales affect distance-based methods</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Standardization (z-score)</li>
                            <li>Min-max normalization</li>
                            <li>Robust scaling</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üîó Connections to Other Topics</h2>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>üìä To Chapter 14-15 (Regression)</h4>
                        <ul>
                            <li>Logistic regression as GLM</li>
                            <li>Regularization techniques</li>
                            <li>Model selection methods</li>
                            <li>Cross-validation strategies</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üéØ To Chapter 22 (Machine Learning)</h4>
                        <ul>
                            <li>Ensemble methods</li>
                            <li>Deep learning architectures</li>
                            <li>Feature learning</li>
                            <li>Modern optimization</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>üìà To Chapter 12 (Bayesian Methods)</h4>
                        <ul>
                            <li>Naive Bayes classifier</li>
                            <li>Bayesian model selection</li>
                            <li>Prior specification</li>
                            <li>Posterior uncertainty</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>üåü Summary and Key Takeaways</h2>

                <div class="algorithm-box">
                    <h4>üéØ Main Messages</h4>
                    <ul>
                        <li><strong>No free lunch:</strong> No single classifier dominates all problems</li>
                        <li><strong>Bias-variance tradeoff:</strong> Central theme in model selection</li>
                        <li><strong>Evaluation is crucial:</strong> Proper assessment essential for deployment</li>
                        <li><strong>Context matters:</strong> Choose method based on problem characteristics</li>
                        <li><strong>Feature engineering:</strong> Often more important than algorithm choice</li>
                        <li><strong>Interpretability vs accuracy:</strong> Important tradeoff to consider</li>
                    </ul>
                </div>

                <div class="visual-demo">
                    <h4>üöÄ Method Selection Guide</h4>
                    <table class="comparison-table">
                        <tr>
                            <th>Method</th>
                            <th>Best For</th>
                            <th>Interpretability</th>
                            <th>Performance</th>
                        </tr>
                        <tr>
                            <td><strong>Logistic Regression</strong></td>
                            <td>Linear relationships, baseline</td>
                            <td>High</td>
                            <td>Good</td>
                        </tr>
                        <tr>
                            <td><strong>Decision Trees</strong></td>
                            <td>Interactions, mixed data</td>
                            <td>High</td>
                            <td>Moderate</td>
                        </tr>
                        <tr>
                            <td><strong>Random Forest</strong></td>
                            <td>General purpose, robust</td>
                            <td>Moderate</td>
                            <td>High</td>
                        </tr>
                        <tr>
                            <td><strong>SVM</strong></td>
                            <td>High dimensions, kernel tricks</td>
                            <td>Low</td>
                            <td>High</td>
                        </tr>
                        <tr>
                            <td><strong>Neural Networks</strong></td>
                            <td>Complex patterns, large data</td>
                            <td>Very Low</td>
                            <td>Very High</td>
                        </tr>
                    </table>
                </div>

                <div class="theorem-box">
                    <h4>üéì For Further Study</h4>
                    <p>Advanced classification topics:</p>
                    <ul>
                        <li><strong>Ensemble methods:</strong> Bagging, boosting, stacking</li>
                        <li><strong>Deep learning:</strong> CNNs, RNNs, transformers</li>
                        <li><strong>Multi-label classification:</strong> Multiple simultaneous labels</li>
                        <li><strong>Online learning:</strong> Streaming data classification</li>
                        <li><strong>Fairness and interpretability:</strong> Ethical AI considerations</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="#" class="nav-btn">‚Üê Chapter 22: Smoothing</a>
            <a href="#" class="nav-btn">Chapter 24: Simulation ‚Üí</a>
        </div>
    </div>
</body>
</html>