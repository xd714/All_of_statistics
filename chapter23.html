<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 23: Classification</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.15);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '📊🎯📈🎯📊';
            position: absolute;
            top: 50%;
            left: -20%;
            transform: translateY(-50%);
            font-size: 6em;
            opacity: 0.1;
            animation: scroll 15s linear infinite;
        }

        @keyframes scroll {
            0% { left: -20%; }
            100% { left: 120%; }
        }

        .header h1 {
            position: relative;
            z-index: 1;
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }

        .header p {
            position: relative;
            z-index: 1;
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 40px;
        }

        .section {
            margin-bottom: 40px;
            padding: 30px;
            border-radius: 15px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            border-left: 5px solid #667eea;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .section::before {
            content: '';
            position: absolute;
            top: -100%;
            right: -100%;
            width: 200px;
            height: 200px;
            background: radial-gradient(circle, rgba(102,126,234,0.1) 0%, transparent 70%);
            border-radius: 50%;
            animation: pulse 4s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); opacity: 0.3; }
            50% { transform: scale(1.2); opacity: 0.1; }
        }

        .section:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.2);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
            display: inline-block;
            position: relative;
            z-index: 1;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            position: relative;
            z-index: 1;
        }

        .classifier-type {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 25px rgba(102, 126, 234, 0.3);
            transition: all 0.3s ease;
            position: relative;
        }

        .classifier-type::before {
            content: '🎯';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 3em;
            opacity: 0.3;
        }

        .classifier-type:hover {
            transform: scale(1.02);
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.4);
        }

        .classifier-type h4 {
            font-size: 1.4em;
            margin-bottom: 15px;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 10px;
        }

        .formula-box {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #f39c12;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            box-shadow: 0 8px 20px rgba(243, 156, 18, 0.2);
            position: relative;
            overflow: hidden;
        }

        .formula-box::before {
            content: 'P(Y|X)';
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 1.5em;
            opacity: 0.2;
            color: #d35400;
            font-style: italic;
        }

        .example-box {
            background: linear-gradient(135deg, #e0c3fc 0%, #9bb5ff 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #9c88ff;
            box-shadow: 0 8px 20px rgba(156, 136, 255, 0.2);
            position: relative;
        }

        .example-box h4 {
            color: #5a4fcf;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .theorem-box {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(255, 107, 107, 0.3);
            position: relative;
        }

        .theorem-box::before {
            content: '∀x';
            position: absolute;
            top: 15px;
            right: 20px;
            font-size: 1.5em;
            opacity: 0.7;
            font-style: italic;
        }

        .theorem-box h4 {
            font-size: 1.3em;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        .highlight {
            background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: bold;
            color: #2c3e50;
            display: inline-block;
            margin: 2px;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }

        .grid-3 {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            margin: 20px 0;
        }

        .comparison-table th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
        }

        .comparison-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #eee;
        }

        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .visual-demo {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
            padding: 25px;
            border-radius: 15px;
            margin: 20px 0;
            border: 3px solid #f39c12;
            text-align: center;
        }

        .sequence-demo {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 15px 0;
            font-size: 1.2em;
            font-weight: bold;
        }

        .sequence-demo .arrow {
            color: #e67e22;
            font-size: 2em;
        }

        .algorithm-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(102, 126, 234, 0.3);
        }

        .algorithm-box h4 {
            font-size: 1.5em;
            margin-bottom: 20px;
            text-align: center;
            border-bottom: 2px solid rgba(255,255,255,0.3);
            padding-bottom: 15px;
        }

        .pitfall-box {
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5a6f 100%);
            color: white;
            padding: 20px;
            border-radius: 15px;
            margin: 15px 0;
            box-shadow: 0 8px 20px rgba(255, 107, 107, 0.3);
        }

        .pitfall-box h4 {
            font-size: 1.2em;
            margin-bottom: 10px;
        }

        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .metric-card {
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            padding: 20px;
            border-radius: 10px;
            border: 2px solid #667eea;
            text-align: center;
        }

        .metric-card h5 {
            color: #2c3e50;
            font-size: 1.1em;
            margin-bottom: 10px;
        }

        ul, ol {
            margin-left: 25px;
            margin-top: 10px;
        }

        li {
            margin-bottom: 8px;
        }

        .nav-buttons {
            display: flex;
            justify-content: space-between;
            padding: 30px 40px;
            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        }

        .nav-btn {
            padding: 12px 25px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: bold;
            transition: all 0.3s ease;
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.3);
        }

        .nav-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 20px rgba(102, 126, 234, 0.4);
        }

        @media (max-width: 768px) {
            .grid-2, .grid-3 {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2em;
            }
            
            .content {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Chapter 23: Classification</h1>
            <p>Predicting Discrete Responses from Data</p>
        </div>

        <div class="content">
            <div class="section">
                <h2>🎯 Introduction to Classification</h2>
                <p>Classification is the problem of predicting a discrete variable Y from another random variable X. We have IID data (X₁, Y₁), ..., (Xₙ, Yₙ) where X ∈ 𝒳 ⊂ ℝᵈ and Y ∈ 𝒴 = {1, 2, ..., k}.</p>
                
                <div class="example-box">
                    <h4>🌟 Real-World Examples</h4>
                    <ul>
                        <li><strong>Email filtering:</strong> Spam vs. Not spam</li>
                        <li><strong>Medical diagnosis:</strong> Disease vs. Healthy</li>
                        <li><strong>Image recognition:</strong> Cat, Dog, Bird classifications</li>
                        <li><strong>Credit scoring:</strong> Good, Bad credit risk</li>
                        <li><strong>Marketing:</strong> Customer segments</li>
                    </ul>
                </div>

                <div class="visual-demo">
                    <h4>🔍 Classification vs. Regression</h4>
                    <table class="comparison-table">
                        <tr>
                            <th>Aspect</th>
                            <th>Classification</th>
                            <th>Regression</th>
                        </tr>
                        <tr>
                            <td><strong>Response</strong></td>
                            <td>Discrete categories</td>
                            <td>Continuous values</td>
                        </tr>
                        <tr>
                            <td><strong>Goal</strong></td>
                            <td>Predict class membership</td>
                            <td>Predict numeric value</td>
                        </tr>
                        <tr>
                            <td><strong>Loss function</strong></td>
                            <td>0-1 loss, log-loss</td>
                            <td>Squared error</td>
                        </tr>
                        <tr>
                            <td><strong>Output</strong></td>
                            <td>Class probabilities</td>
                            <td>Point estimates</td>
                        </tr>
                    </table>
                </div>
            </div>

            <div class="section">
                <h2>⚖️ Risk and the Bayes Classifier</h2>

                <div class="theorem-box">
                    <h4>📊 Risk Function</h4>
                    <p>The <strong>risk</strong> (misclassification error) of classifier h is:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        R(h) = P(h(X) ≠ Y) = E[I(h(X) ≠ Y)]
                    </div>
                </div>

                <div class="algorithm-box">
                    <h4>🎯 Bayes Classifier</h4>
                    <p>The optimal classifier that minimizes risk:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        h*(x) = argmax P(Y = j|X = x)
                                  j
                    </div>
                    <p><strong>Bayes risk:</strong> R* = R(h*) (minimum possible risk)</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>🔍 Posterior Probabilities</h4>
                        <p>Let η_j(x) = P(Y = j|X = x)</p>
                        <p><strong>Properties:</strong></p>
                        <ul>
                            <li>∑_j η_j(x) = 1</li>
                            <li>η_j(x) ≥ 0</li>
                            <li>h*(x) = argmax η_j(x)</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>🎲 Binary Classification</h4>
                        <p>For k = 2 classes:</p>
                        <div class="formula-box">
                            h*(x) = {1 if η₁(x) > 1/2
                                    {0 if η₁(x) < 1/2
                        </div>
                        <p><strong>Decision boundary:</strong> {x : η₁(x) = 1/2}</p>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>📏 Linear and Quadratic Discriminant Analysis</h2>

                <div class="classifier-type">
                    <h4>📊 Linear Discriminant Analysis (LDA)</h4>
                    <p><strong>Assumptions:</strong></p>
                    <ul>
                        <li>X|Y = j ~ N(μⱼ, Σ) (same covariance)</li>
                        <li>P(Y = j) = πⱼ</li>
                    </ul>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        Discriminant functions:
                        <br>δⱼ(x) = log πⱼ - (1/2)μⱼᵀΣ⁻¹μⱼ + xᵀΣ⁻¹μⱼ
                    </div>
                    <p><strong>Classification:</strong> h(x) = argmax δⱼ(x)</p>
                </div>

                <div class="classifier-type">
                    <h4>📈 Quadratic Discriminant Analysis (QDA)</h4>
                    <p><strong>Assumption:</strong> X|Y = j ~ N(μⱼ, Σⱼ) (different covariances)</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        δⱼ(x) = log πⱼ - (1/2)log|Σⱼ| - (1/2)(x - μⱼ)ᵀΣⱼ⁻¹(x - μⱼ)
                    </div>
                    <p><strong>Decision boundaries:</strong> Quadratic in x</p>
                </div>

                <div class="visual-demo">
                    <h4>⚖️ LDA vs QDA Trade-off</h4>
                    <div class="grid-2">
                        <div style="background: rgba(102, 126, 234, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>📊 LDA</h5>
                            <p><strong>Lower variance</strong></p>
                            <p><strong>Higher bias</strong></p>
                            <p>Assumes equal covariances</p>
                        </div>
                        <div style="background: rgba(255, 107, 107, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>📈 QDA</h5>
                            <p><strong>Higher variance</strong></p>
                            <p><strong>Lower bias</strong></p>
                            <p>More flexible model</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>📊 Logistic Regression</h2>

                <div class="algorithm-box">
                    <h4>🎯 Binary Logistic Model</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        log(P(Y = 1|X = x)/(P(Y = 0|X = x))) = β₀ + βᵀx
                    </div>
                    <p><strong>Probability:</strong></p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        P(Y = 1|X = x) = exp(β₀ + βᵀx)/(1 + exp(β₀ + βᵀx))
                    </div>
                </div>

                <div class="classifier-type">
                    <h4>🔢 Multinomial Logistic Regression</h4>
                    <p>For k classes, use k-1 log-odds:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2); border: 1px solid rgba(255,255,255,0.3);">
                        log(P(Y = j|X = x)/P(Y = k|X = x)) = β₀ⱼ + βⱼᵀx
                        <br>for j = 1, ..., k-1
                    </div>
                    <p><strong>No closed form solution:</strong> Use iterative methods (Newton-Raphson, IRLS)</p>
                </div>

                <div class="example-box">
                    <h4>📈 Maximum Likelihood Estimation</h4>
                    <div class="formula-box">
                        ℓ(β) = ∑ᵢ₌₁ⁿ ∑ⱼ₌₁ᵏ Yᵢⱼ log P(Y = j|Xᵢ, β)
                    </div>
                    <p>where Yᵢⱼ = I(Yᵢ = j)</p>
                    <p><strong>Advantages:</strong> Flexible, interpretable coefficients, probabilistic output</p>
                </div>
            </div>

            <div class="section">
                <h2>🌳 Classification Trees</h2>

                <div class="algorithm-box">
                    <h4>🔄 Recursive Binary Splitting</h4>
                    <ol>
                        <li>Find best split (feature + threshold)</li>
                        <li>Split data into two subsets</li>
                        <li>Repeat recursively on each subset</li>
                        <li>Stop when stopping criterion met</li>
                    </ol>
                </div>

                <div class="grid-3">
                    <div class="classifier-type">
                        <h4>📊 Misclassification Rate</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ∑ⱼ I(ĵ ≠ j)p̂ⱼ
                        </div>
                        <p>Simple but not differentiable</p>
                    </div>

                    <div class="classifier-type">
                        <h4>🎯 Gini Index</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ∑ⱼ p̂ⱼ(1 - p̂ⱼ) = 1 - ∑ⱼ p̂ⱼ²
                        </div>
                        <p>Measures node "impurity"</p>
                    </div>

                    <div class="classifier-type">
                        <h4>📈 Entropy</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            -∑ⱼ p̂ⱼ log p̂ⱼ
                        </div>
                        <p>Information-theoretic measure</p>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>✂️ Cost-Complexity Pruning</h4>
                    <p>Large trees overfit. Use cost-complexity criterion:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        Cα(T) = ∑ₜ Nₜ Qₜ + α|T|
                    </div>
                    <p>Where Nₜ = observations in leaf t, Qₜ = misclassification rate, |T| = number of leaves</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>✅ Tree Advantages</h4>
                        <ul>
                            <li>Easy to interpret</li>
                            <li>Handles mixed data types</li>
                            <li>Automatic feature selection</li>
                            <li>Captures interactions</li>
                            <li>No distributional assumptions</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>⚠️ Tree Disadvantages</h4>
                        <ul>
                            <li>High variance</li>
                            <li>Biased toward variables with more levels</li>
                            <li>Difficulty capturing linear relationships</li>
                            <li>Greedy algorithm (not globally optimal)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🎯 k-Nearest Neighbors (k-NN)</h2>

                <div class="algorithm-box">
                    <h4>🔍 k-NN Algorithm</h4>
                    <ol>
                        <li>Find k nearest neighbors to x in training set</li>
                        <li>Classify x as majority class among k neighbors</li>
                    </ol>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        ĥ(x) = argmax ∑ᵢ∈Nₖ(x) I(yᵢ = j)
                               j
                    </div>
                </div>

                <div class="grid-3">
                    <div class="classifier-type">
                        <h4>📏 Euclidean Distance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ||x - xᵢ||₂ = √∑ⱼ(xⱼ - xᵢⱼ)²
                        </div>
                    </div>

                    <div class="classifier-type">
                        <h4>🚶 Manhattan Distance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ||x - xᵢ||₁ = ∑ⱼ|xⱼ - xᵢⱼ|
                        </div>
                    </div>

                    <div class="classifier-type">
                        <h4>⚡ Minkowski Distance</h4>
                        <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                            ||x - xᵢ||ₚ = (∑ⱼ|xⱼ - xᵢⱼ|ᵖ)^(1/p)
                        </div>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>📊 Theoretical Properties</h4>
                    <p><strong>Consistency:</strong> As n → ∞ and k → ∞ with k/n → 0:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        R(ĥₖ) → R* (Bayes risk)
                    </div>
                    <p><strong>Curse of dimensionality:</strong> Performance degrades in high dimensions</p>
                </div>

                <div class="visual-demo">
                    <h4>🎛️ Choosing k</h4>
                    <div class="grid-3">
                        <div style="background: rgba(255, 107, 107, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>Small k</h5>
                            <p>Low bias</p>
                            <p>High variance</p>
                        </div>
                        <div style="background: rgba(107, 207, 127, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>k = √n</h5>
                            <p>Common choice</p>
                            <p>Good balance</p>
                        </div>
                        <div style="background: rgba(102, 126, 234, 0.1); padding: 15px; border-radius: 10px;">
                            <h5>Large k</h5>
                            <p>High bias</p>
                            <p>Low variance</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>⚡ Support Vector Machines (SVM)</h2>

                <div class="algorithm-box">
                    <h4>📏 Linear SVM (Separable Case)</h4>
                    <p><strong>Goal:</strong> Find hyperplane with maximum margin</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        minimize (1/2)||w||²
                        subject to yᵢ(wᵀxᵢ + b) ≥ 1, i = 1, ..., n
                    </div>
                    <p><strong>Margin:</strong> 1/||w|| (distance to nearest point)</p>
                </div>

                <div class="classifier-type">
                    <h4>🔧 Soft Margin SVM</h4>
                    <p>Allow some misclassification with slack variables:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        minimize (1/2)||w||² + C∑ᵢξᵢ
                        subject to yᵢ(wᵀxᵢ + b) ≥ 1 - ξᵢ, ξᵢ ≥ 0
                    </div>
                    <p><strong>C parameter:</strong> Controls trade-off between margin and violations</p>
                </div>

                <div class="theorem-box">
                    <h4>🎯 Kernel Trick</h4>
                    <p>Transform to higher-dimensional space using kernel functions:</p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        K(x, x') = φ(x)ᵀφ(x')
                    </div>
                    <p><strong>Decision function:</strong></p>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        f(x) = ∑ᵢ αᵢyᵢK(xᵢ, x) + b
                    </div>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>🔧 Common Kernels</h4>
                        <ul>
                            <li><strong>Linear:</strong> K(x, x') = xᵀx'</li>
                            <li><strong>Polynomial:</strong> K(x, x') = (γxᵀx' + r)^d</li>
                            <li><strong>RBF:</strong> K(x, x') = exp(-γ||x - x'||²)</li>
                            <li><strong>Sigmoid:</strong> K(x, x') = tanh(γxᵀx' + r)</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>⭐ SVM Properties</h4>
                        <ul>
                            <li><strong>Maximum margin:</strong> Unique solution</li>
                            <li><strong>Sparse:</strong> Only support vectors matter</li>
                            <li><strong>Kernel trick:</strong> Handles nonlinear boundaries</li>
                            <li><strong>Regularization:</strong> C controls overfitting</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🧠 Neural Networks</h2>

                <div class="classifier-type">
                    <h4>⚡ Single Layer Perceptron</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        f(x) = σ(w₀ + ∑ⱼwⱼxⱼ)
                    </div>
                    <p>Where σ is activation function (sigmoid, tanh, ReLU)</p>
                </div>

                <div class="algorithm-box">
                    <h4>🏗️ Multi-Layer Perceptron</h4>
                    <div class="formula-box" style="background: rgba(255,255,255,0.2);">
                        Hidden layer: h = σ(W₁x + b₁)
                        <br>Output layer: y = σ(W₂h + b₂)
                    </div>
                    <p><strong>Training:</strong> Backpropagation algorithm</p>
                </div>

                <div class="grid-2">
                    <div class="example-box">
                        <h4>✅ Neural Network Advantages</h4>
                        <ul>
                            <li>Universal approximation capability</li>
                            <li>Automatic feature learning</li>
                            <li>Handles complex nonlinear patterns</li>
                            <li>Scalable to large datasets</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>⚠️ Neural Network Challenges</h4>
                        <ul>
                            <li>Many hyperparameters to tune</li>
                            <li>Prone to overfitting</li>
                            <li>Black box (low interpretability)</li>
                            <li>Requires large amounts of data</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>📊 Model Evaluation and Selection</h2>

                <div class="algorithm-box">
                    <h4>🔄 Cross-Validation</h4>
                    <ol>
                        <li>Divide data into k folds</li>
                        <li>For each fold: train on k-1 folds, test on remaining</li>
                        <li>Average test errors</li>
                    </ol>
                    <p><strong>Leave-one-out CV:</strong> Special case with k = n</p>
                </div>

                <div class="visual-demo">
                    <h4>📈 Confusion Matrix</h4>
                    <table class="comparison-table">
                        <tr>
                            <th></th>
                            <th colspan="2">Predicted</th>
                        </tr>
                        <tr>
                            <th>Actual</th>
                            <th>Positive</th>
                            <th>Negative</th>
                        </tr>
                        <tr>
                            <td><strong>Positive</strong></td>
                            <td style="background: #d4edda;">TP</td>
                            <td style="background: #f8d7da;">FN</td>
                        </tr>
                        <tr>
                            <td><strong>Negative</strong></td>
                            <td style="background: #f8d7da;">FP</td>
                            <td style="background: #d4edda;">TN</td>
                        </tr>
                    </table>
                </div>

                <div class="metrics-grid">
                    <div class="metric-card">
                        <h5>🎯 Accuracy</h5>
                        <div class="formula-box">
                            (TP + TN)/(TP + TN + FP + FN)
                        </div>
                    </div>
                    <div class="metric-card">
                        <h5>🔍 Precision</h5>
                        <div class="formula-box">
                            TP/(TP + FP)
                        </div>
                    </div>
                    <div class="metric-card">
                        <h5>📡 Recall</h5>
                        <div class="formula-box">
                            TP/(TP + FN)
                        </div>
                    </div>
                    <div class="metric-card">
                        <h5>⚖️ F1-Score</h5>
                        <div class="formula-box">
                            2 × (Precision × Recall)/(Precision + Recall)
                        </div>
                    </div>
                </div>

                <div class="theorem-box">
                    <h4>📈 ROC and AUC</h4>
                    <p><strong>ROC curve:</strong> Plot of True Positive Rate vs False Positive Rate</p>
                    <p><strong>AUC:</strong> Area Under the ROC Curve</p>
                    <ul>
                        <li>AUC = 0.5: Random classifier</li>
                        <li>AUC = 1.0: Perfect classifier</li>
                        <li>Higher AUC = Better performance</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>⚠️ Common Pitfalls and Practical Considerations</h2>

                <div class="grid-2">
                    <div class="pitfall-box">
                        <h4>❌ Data Leakage</h4>
                        <p><strong>Problem:</strong> Information from future/target leaks into features</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Careful feature engineering</li>
                            <li>Temporal validation for time series</li>
                            <li>Strict train/validation/test splits</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>⚖️ Imbalanced Classes</h4>
                        <p><strong>Problem:</strong> Some classes much more frequent</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Resampling (SMOTE, undersampling)</li>
                            <li>Cost-sensitive learning</li>
                            <li>Different evaluation metrics</li>
                        </ul>
                    </div>
                </div>

                <div class="grid-2">
                    <div class="pitfall-box">
                        <h4>🎯 Overfitting</h4>
                        <p><strong>Symptoms:</strong> High training accuracy, poor test performance</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Regularization (L1, L2)</li>
                            <li>Cross-validation</li>
                            <li>More data</li>
                            <li>Simpler models</li>
                        </ul>
                    </div>

                    <div class="pitfall-box">
                        <h4>📏 Feature Scaling</h4>
                        <p><strong>Problem:</strong> Features on different scales affect distance-based methods</p>
                        <p><strong>Solutions:</strong></p>
                        <ul>
                            <li>Standardization (z-score)</li>
                            <li>Min-max normalization</li>
                            <li>Robust scaling</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🔗 Connections to Other Topics</h2>

                <div class="grid-3">
                    <div class="example-box">
                        <h4>📊 To Chapter 14-15 (Regression)</h4>
                        <ul>
                            <li>Logistic regression as GLM</li>
                            <li>Regularization techniques</li>
                            <li>Model selection methods</li>
                            <li>Cross-validation strategies</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>🎯 To Chapter 22 (Machine Learning)</h4>
                        <ul>
                            <li>Ensemble methods</li>
                            <li>Deep learning architectures</li>
                            <li>Feature learning</li>
                            <li>Modern optimization</li>
                        </ul>
                    </div>

                    <div class="example-box">
                        <h4>📈 To Chapter 12 (Bayesian Methods)</h4>
                        <ul>
                            <li>Naive Bayes classifier</li>
                            <li>Bayesian model selection</li>
                            <li>Prior specification</li>
                            <li>Posterior uncertainty</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section">
                <h2>🌟 Summary and Key Takeaways</h2>

                <div class="algorithm-box">
                    <h4>🎯 Main Messages</h4>
                    <ul>
                        <li><strong>No free lunch:</strong> No single classifier dominates all problems</li>
                        <li><strong>Bias-variance tradeoff:</strong> Central theme in model selection</li>
                        <li><strong>Evaluation is crucial:</strong> Proper assessment essential for deployment</li>
                        <li><strong>Context matters:</strong> Choose method based on problem characteristics</li>
                        <li><strong>Feature engineering:</strong> Often more important than algorithm choice</li>
                        <li><strong>Interpretability vs accuracy:</strong> Important tradeoff to consider</li>
                    </ul>
                </div>

                <div class="visual-demo">
                    <h4>🚀 Method Selection Guide</h4>
                    <table class="comparison-table">
                        <tr>
                            <th>Method</th>
                            <th>Best For</th>
                            <th>Interpretability</th>
                            <th>Performance</th>
                        </tr>
                        <tr>
                            <td><strong>Logistic Regression</strong></td>
                            <td>Linear relationships, baseline</td>
                            <td>High</td>
                            <td>Good</td>
                        </tr>
                        <tr>
                            <td><strong>Decision Trees</strong></td>
                            <td>Interactions, mixed data</td>
                            <td>High</td>
                            <td>Moderate</td>
                        </tr>
                        <tr>
                            <td><strong>Random Forest</strong></td>
                            <td>General purpose, robust</td>
                            <td>Moderate</td>
                            <td>High</td>
                        </tr>
                        <tr>
                            <td><strong>SVM</strong></td>
                            <td>High dimensions, kernel tricks</td>
                            <td>Low</td>
                            <td>High</td>
                        </tr>
                        <tr>
                            <td><strong>Neural Networks</strong></td>
                            <td>Complex patterns, large data</td>
                            <td>Very Low</td>
                            <td>Very High</td>
                        </tr>
                    </table>
                </div>

                <div class="theorem-box">
                    <h4>🎓 For Further Study</h4>
                    <p>Advanced classification topics:</p>
                    <ul>
                        <li><strong>Ensemble methods:</strong> Bagging, boosting, stacking</li>
                        <li><strong>Deep learning:</strong> CNNs, RNNs, transformers</li>
                        <li><strong>Multi-label classification:</strong> Multiple simultaneous labels</li>
                        <li><strong>Online learning:</strong> Streaming data classification</li>
                        <li><strong>Fairness and interpretability:</strong> Ethical AI considerations</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="nav-buttons">
            <a href="#" class="nav-btn">← Chapter 22: Smoothing</a>
            <a href="#" class="nav-btn">Chapter 24: Simulation →</a>
        </div>
    </div>
</body>
</html>